{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /notebooks\n",
    "!git clone https://github.com/speechbrain/speechbrain/\n",
    "%cd /notebooks/speechbrain\n",
    "!git pull\n",
    "%pip install -r requirements.txt\n",
    "%pip install -e . \n",
    "%cd /notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /notebooks/\n",
    "!git clone https://github.com/huggingface/transformers.git\n",
    "%cd transformers\n",
    "!git pull\n",
    "%pip install -e .\n",
    "%cd /notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --force-reinstall   torch torchvision  torchaudio --extra-index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests moviepy librosa pytube pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def download_video_at_url(url,output_path):\n",
    "    # Utilisez la commande ! pour télécharger le fichier en utilisant aria2c\n",
    "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M -o {output_path} {url}\n",
    "def split_audio_into_chunks(file_path, output_dir,minutes=5):\n",
    "    \"\"\"\n",
    "    Splits an audio file into five-minute chunks and saves them to the output directory.\n",
    "    \n",
    "    :param file_path: The path to the input audio file.\n",
    "    :param output_dir: The directory where the audio chunks will be saved.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_file(file_path)\n",
    "    \n",
    "    # Define the length of each chunk in milliseconds (5 minutes)\n",
    "    chunk_length_ms = minutes * 60 * 1000\n",
    "    \n",
    "    # Split the audio into chunks\n",
    "    chunks = []\n",
    "    for i in range(0, len(audio), chunk_length_ms):\n",
    "        chunk = audio[i:i + chunk_length_ms]\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    # Export each chunk as a separate audio file\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_filename = f\"{output_dir}/chunk_{i}.wav\"  # You can change the format to '.mp3' if needed\n",
    "        chunk.export(chunk_filename, format=\"wav\")  # Change 'wav' to 'mp3' if exporting as MP3\n",
    "        print(f\"Exported {chunk_filename}\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to download YouTube video\n",
    "def download_youtube_video(youtube_url, output_path):\n",
    "    !/root/miniconda3/envs/tts/bin/yt-dlp  -o {output_path} {youtube_url}\n",
    "    return output_path\n",
    "\n",
    "# Function to extract audio from video\n",
    "def extract_audio_from_video(video_path, audio_output_path):\n",
    "    with VideoFileClip(video_path) as video:\n",
    "        audio = video.audio\n",
    "        audio.write_audiofile(audio_output_path, codec='aac')\n",
    "\n",
    "# Function to resample audio\n",
    "def resample_audio(input_audio_path, output_audio_path, target_sr):\n",
    "    signal, sr_orig = librosa.load(input_audio_path, sr=None)  # Load audio without resampling\n",
    "    signal_resampled = librosa.resample(signal, orig_sr=sr_orig, target_sr=target_sr)  # Resample audio\n",
    "    sf.write(output_audio_path, signal_resampled, target_sr)  # Save resampled audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "from moviepy.editor import VideoFileClip\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio\n",
    "\n",
    "# URL of the YouTube video\n",
    "youtube_url = 'https://www.youtube.com/watch?v=OT_KEqdJvUA'\n",
    "\n",
    "# Paths for the downloaded video and audio files\n",
    "video_path = 'downloaded_video.mp4'\n",
    "audio_path = 'extracted_audio.aac'\n",
    "resampled_audio_path = 'resampled_audio.wav'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!rm /notebooks/{video_path}\n",
    "# Download the video from YouTube\n",
    "download_youtube_video(youtube_url, video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target sampling rate\n",
    "target_sampling_rate = 24000\n",
    "\n",
    "# Extract audio from the downloaded video\n",
    "extract_audio_from_video(video_path, audio_path)\n",
    "\n",
    "# Resample the extracted audio to the target sampling rate\n",
    "resample_audio(audio_path, resampled_audio_path, target_sampling_rate)\n",
    "\n",
    "print(f\"Resampled audio saved to: {resampled_audio_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install cython speechbrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyannote.audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./reseampled_audio\n",
    "split_audio_into_chunks('resampled_audio.wav', './reseampled_audio',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.pretrained import SepformerSeparation as separator\n",
    "import torchaudio\n",
    "\n",
    "model = separator.from_hparams(source=\"speechbrain/sepformer-libri3mix\", savedir='pretrained_models/sepformer-libri3mix',run_opts={\"device\":\"cuda\"} )\n",
    "\n",
    "est_sources = model.separate_file(path='resampled_audio.wav') \n",
    "\n",
    "torchaudio.save(\"source1hat.wav\", est_sources[:, :, 0].detach().cpu(), 8000)\n",
    "torchaudio.save(\"source2hat.wav\", est_sources[:, :, 1].detach().cpu(), 8000)\n",
    "torchaudio.save(\"source3hat.wav\", est_sources[:, :, 2].detach().cpu(), 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio('source3hat.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline, Audio\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "import os\n",
    "\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\")\n",
    "pipeline.to(torch.device(\"cuda\"))\n",
    "audio_path=\"resampled_audio.wav\"\n",
    "# apply pretrained pipeline\n",
    "diarization = pipeline(audio_path)\n",
    "audio = AudioSegment.from_wav(audio_path)\n",
    "output_dir = \"by_speaker\"\n",
    "!rm -rf {output_dir}\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "speakers = {}\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    start_ms = int(turn.start * 1000)\n",
    "    end_ms = int(turn.end * 1000)\n",
    "    \n",
    "    speaker_audio = audio[start_ms:end_ms]\n",
    "    if speaker not in speakers:\n",
    "        speakers[speaker] = speaker_audio\n",
    "    else:\n",
    "        speakers[speaker] += speaker_audio\n",
    "\n",
    "for speaker, audio in speakers.items():\n",
    "    audio.export(os.path.join(output_dir, f\"{speaker}.wav\"), format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./reseampled_audio\n",
    "split_audio_into_chunks('/notebooks/by_speaker/SPEAKER_01.wav', './reseampled_audio',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Load audio file\n",
    "audio = AudioSegment.from_wav(\"/notebooks/by_speaker/SPEAKER_01.wav\") \n",
    "\n",
    "# Get start and end times in milliseconds\n",
    "start_time = 0.05*60*1000 + 0*1000 # 2 min 54 sec\n",
    "end_time = 0.15*60*1000 + 0*1000  # 4 min\n",
    "\n",
    "# Extract segment\n",
    "segment = audio[start_time:end_time]\n",
    "\n",
    "# Resample to 24000 Hz\n",
    "segment = segment.set_frame_rate(16000)\n",
    "\n",
    "# Convert to mono\n",
    "segment = segment.set_channels(1)\n",
    "\n",
    "# Export to wav file\n",
    "segment.export(\"segment_24kmono.wav\", format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.pretrained import SepformerSeparation as separator\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "model = separator.from_hparams(\n",
    "source=\"speechbrain/sepformer-dns4-16k-enhancement\", savedir='pretrained_models/sepformer-dns4-16k-enhancement',run_opts={\"device\":\"cuda\"} )\n",
    "\n",
    "# for custom file, change path\n",
    "est_sources = model.separate_file(path='/notebooks/by_speaker/SPEAKER_01.wav') \n",
    "\n",
    "torchaudio.save(\"source2hat.wav\", est_sources[:, :, 0].detach().cpu(), 16000)\n",
    "#torchaudio.save(\"source2hat.wav\", est_sources[:, :, 1].detach().cpu(), 16000)\n",
    "Audio(est_sources[:, :, 0].detach().cpu(),rate=16000)\n",
    "#Audio(est_sources[:, :, 1].detach().cpu(),rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(est_sources[:, :, 1].detach().cpu(),rate=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(est_sources[:, :, 0].detach().cpu(),rate=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from speechbrain.pretrained import WaveformEnhancement\n",
    "\n",
    "enhance_model = WaveformEnhancement.from_hparams(\n",
    "    source=\"speechbrain/mtl-mimic-voicebank\",\n",
    "    savedir=\"pretrained_models/mtl-mimic-voicebank\",\n",
    ")\n",
    "enhanced = enhance_model.enhance_file(\"segment_24kmono.wav\")\n",
    "\n",
    "# Saving enhanced signal on disk\n",
    "torchaudio.save('enhanced.wav', enhanced.unsqueeze(0).cpu(), 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio('enhanced.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from speechbrain.pretrained import SpectralMaskEnhancement\n",
    "\n",
    "enhance_model = SpectralMaskEnhancement.from_hparams(\n",
    "    source=\"speechbrain/metricgan-plus-voicebank\",\n",
    "    savedir=\"pretrained_models/metricgan-plus-voicebank\",\n",
    ")\n",
    "\n",
    "# Load and add fake batch dimension\n",
    "noisy = enhance_model.load_audio(\n",
    "    \"source3hat.wav\"\n",
    ").unsqueeze(0)\n",
    "\n",
    "# Add relative length tensor\n",
    "enhanced = enhance_model.enhance_batch(noisy, lengths=torch.tensor([1.]))\n",
    "\n",
    "# Saving enhanced signal on disk\n",
    "torchaudio.save('enhanced.wav', enhanced.cpu(), 16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts",
   "language": "python",
   "name": "tts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
