{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kaggle\n",
    "%pip install  --pre torch torchvision  torchaudio --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install numpy pandas fastai  transformers datasets sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/notebooks/Titanic')  # Update this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import tensor\n",
    "import torch.nn.functional as F\n",
    "from fastai.data.transforms import RandomSplitter\n",
    "from pathlib import Path\n",
    "\n",
    "# Load your data here\n",
    "\n",
    "df = pd.read_csv(path/'train.csv')\n",
    "\n",
    "# Data preparation\n",
    "modes = df.mode().iloc[0]\n",
    "df.fillna(modes, inplace=True)\n",
    "\n",
    "# Ensure all columns are numeric or converted to numeric\n",
    "df['LogFare'] = np.log(df['Fare'] + 1)\n",
    "df = pd.get_dummies(df, columns=[\"Sex\", \"Pclass\", \"Embarked\"])\n",
    "\n",
    "# Defining independent and dependent variables\n",
    "indep_cols = ['Age', 'SibSp', 'Parch', 'LogFare', 'Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\n",
    "t_dep = tensor(df.Survived.values)\n",
    "t_indep = tensor(df[indep_cols].values.astype(np.float32))  # Ensure all values are float\n",
    "\n",
    "# Normalizing the data\n",
    "vals, _ = t_indep.max(dim=0)\n",
    "t_indep = t_indep / vals\n",
    "\n",
    "# Splitting data into training and validation sets\n",
    "trn_split, val_split = RandomSplitter(seed=42)(df)\n",
    "trn_indep, val_indep = t_indep[trn_split], t_indep[val_split]\n",
    "trn_dep, val_dep = t_dep[trn_split], t_dep[val_split]\n",
    "\n",
    "# Function definitions\n",
    "def calc_preds(coeffs, indeps): \n",
    "    return (indeps @ coeffs).sum(axis=1)\n",
    "\n",
    "def calc_loss(coeffs, indeps, deps): \n",
    "    return torch.abs(calc_preds(coeffs, indeps) - deps).mean()\n",
    "\n",
    "def update_coeffs(coeffs, lr):\n",
    "    coeffs.sub_(coeffs.grad * lr)\n",
    "    coeffs.grad.zero_()\n",
    "\n",
    "def one_epoch(coeffs, lr):\n",
    "    loss = calc_loss(coeffs, trn_indep, trn_dep)\n",
    "    loss.backward()\n",
    "    with torch.no_grad(): \n",
    "        update_coeffs(coeffs, lr)\n",
    "\n",
    "def init_coeffs():\n",
    "    return (torch.rand(t_indep.shape[1], 1) - 0.5).requires_grad_()\n",
    "\n",
    "# Training the model\n",
    "def train_model(epochs=30, lr=0.01):\n",
    "    coeffs = init_coeffs()\n",
    "    for _ in range(epochs): \n",
    "        one_epoch(coeffs, lr=lr)\n",
    "    return coeffs\n",
    "\n",
    "coeffs = train_model(lr=0.2)\n",
    "\n",
    "# Calculating accuracy\n",
    "def acc(coeffs):\n",
    "    preds = calc_preds(coeffs, val_indep)\n",
    "    return (val_dep.bool() == (preds > 0.5)).float().mean()\n",
    "\n",
    "print(\"Accuracy:\", acc(coeffs).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn, tensor\n",
    "import torch.nn.functional as F\n",
    "from fastai.data.transforms import RandomSplitter\n",
    "from pathlib import Path\n",
    "\n",
    "# Load your data here\n",
    "\n",
    "df = pd.read_csv(path/'train.csv')\n",
    "\n",
    "# Data preparation\n",
    "modes = df.mode().iloc[0]\n",
    "df.fillna(modes, inplace=True)\n",
    "df['LogFare'] = np.log(df['Fare'] + 1)\n",
    "df = pd.get_dummies(df, columns=[\"Sex\", \"Pclass\", \"Embarked\"])\n",
    "\n",
    "# Defining independent and dependent variables\n",
    "indep_cols = ['Age', 'SibSp', 'Parch', 'LogFare', 'Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\n",
    "t_dep = tensor(df.Survived.values, dtype=torch.float32)\n",
    "t_indep = tensor(df[indep_cols].values.astype(np.float32))\n",
    "\n",
    "# Normalizing the data\n",
    "vals, _ = t_indep.max(dim=0)\n",
    "t_indep = t_indep / vals\n",
    "\n",
    "# Splitting data into training and validation sets\n",
    "trn_split, val_split = RandomSplitter(seed=42)(df)\n",
    "trn_indep, val_indep = t_indep[trn_split], t_indep[val_split]\n",
    "trn_dep, val_dep = t_dep[trn_split], t_dep[val_split]\n",
    "\n",
    "# Neural Network\n",
    "class TitanicNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(TitanicNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.layer1(x))\n",
    "        x = torch.sigmoid(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the NN\n",
    "input_size = len(indep_cols)\n",
    "hidden_size = 10  # This is adjustable\n",
    "output_size = 1\n",
    "model = TitanicNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training the model\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(trn_indep)\n",
    "    loss = criterion(outputs, trn_dep.reshape(-1, 1))\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, loss.item()))\n",
    "\n",
    "# Testing the model\n",
    "with torch.no_grad():\n",
    "    val_outputs = model(val_indep)\n",
    "    val_outputs = (val_outputs > 0.5).float()\n",
    "    accuracy = (val_outputs.reshape(-1) == val_dep).float().mean()\n",
    "    print(f'Accuracy: {accuracy.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a dataset from Hugging Face (e.g., \"wikitext\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenized_texts = [torch.tensor(tokenizer.encode(text[\"text\"])) for text in dataset.select(range(1000))]\n",
    "\n",
    "# Padding sequences and creating dataloader\n",
    "def collate_batch(batch):\n",
    "    return pad_sequence(batch, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "dataloader = DataLoader(tokenized_texts, batch_size=32, collate_fn=collate_batch)\n",
    "\n",
    "# Define a simple RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.rnn(x)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = RNNModel(vocab_size, embed_size, hidden_size, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output.view(-1, vocab_size), batch.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn, tensor\n",
    "import torch.nn.functional as F\n",
    "from fastai.data.transforms import RandomSplitter\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(model, start_str, length, temperature=1.0):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Tokenize and encode the start string\n",
    "    input_ids = tokenizer.encode(start_str, return_tensors='pt')\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            outputs = model(input_ids)\n",
    "            predictions = outputs[:, -1, :]\n",
    "\n",
    "            # Apply temperature to predictions and sample a token\n",
    "            predictions = predictions / temperature\n",
    "            predicted_id = torch.multinomial(F.softmax(predictions, dim=-1), num_samples=1)\n",
    "\n",
    "            # Append predicted token to the input sequence and continue\n",
    "            input_ids = torch.cat([input_ids, predicted_id], dim=-1)\n",
    "\n",
    "    # Decode the tokens and return the text\n",
    "    return tokenizer.decode(input_ids[0])\n",
    "\n",
    "# Example usage\n",
    "start_str = \"The weather today\"\n",
    "generated_text = generate_text(model, start_str, length=50, temperature=0.7)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "# Load a dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "text = \" \".join(dataset[\"text\"][:1000])  # Using a larger subset for better training\n",
    "\n",
    "# Save the text to a file\n",
    "with open(\"text_data.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(text)\n",
    "\n",
    "# Train a SentencePiece tokenizer\n",
    "spm.SentencePieceTrainer.train(input='text_data.txt', model_prefix='sp', vocab_size=5000, model_type='bpe')\n",
    "\n",
    "# Load the trained model\n",
    "sp = spm.SentencePieceProcessor(model_file='sp.model')\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_texts = [torch.tensor(sp.encode(text)) for text in dataset[\"text\"][:1000]]\n",
    "\n",
    "# Padding sequences and creating dataloader\n",
    "def collate_batch(batch):\n",
    "    return pad_sequence(batch, padding_value=sp.pad_id()).long()  # Convert to LongTensor\n",
    "\n",
    "dataloader = DataLoader(tokenized_texts, batch_size=32, collate_fn=collate_batch)\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMTitanic(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(LSTMTitanic, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(num_layers, batch_size, hidden_size),\n",
    "                torch.zeros(num_layers, batch_size, hidden_size))\n",
    "        \n",
    "        \n",
    "max_token_id = max([token.max().item() for token in tokenized_texts if len(token) > 0])\n",
    "print(f\"Maximum token ID in the data: {max_token_id}\")\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = max(max_token_id + 1, vocab_size)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LSTMTitanic(vocab_size, embed_size, hidden_size, num_layers)  # Use this vocab size\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    hidden = model.init_hidden(32)\n",
    "    for batch in dataloader:\n",
    "        # Detach hidden states\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(batch, hidden)\n",
    "        loss = criterion(output.view(-1, vocab_size), batch.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "raw_datasets[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"text\"][i : i + 1000]\n",
    "        for i in range(0, len(raw_datasets[\"text\"]), 1000)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import transformers\n",
    "tk_tokenizer = SentencePieceBPETokenizer()\n",
    "def batch_iterator(batch_size=1000):\n",
    "    for i in range(0, len(raw_datasets), batch_size):\n",
    "        yield raw_datasets[i : i + batch_size][\"text\"]\n",
    "special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<cls>\", \"<sep>\", \"<mask>\"]\n",
    "\n",
    "tk_tokenizer.train_from_iterator(\n",
    "    training_corpus,\n",
    "    vocab_size=30_000,\n",
    "    min_frequency=5,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=500,\n",
    "    special_tokens=special_tokens\n",
    ")\n",
    "tk_tokenizer.save(\"./tk-tokenizer-test\")\n",
    "tokenizer = transformers.PreTrainedTokenizerFast(tokenizer_object=tk_tokenizer, model_max_length=2048, special_tokens=special_tokens)\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.bos_token_id = tk_tokenizer.token_to_id(\"<s>\")\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.pad_token_id = tk_tokenizer.token_to_id(\"<pad>\")\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.eos_token_id = tk_tokenizer.token_to_id(\"</s>\")\n",
    "tokenizer.unk_token = \"<unk>\"\n",
    "tokenizer.unk_token_id = tk_tokenizer.token_to_id(\"<unk>\")\n",
    "tokenizer.cls_token = \"<cls>\"\n",
    "tokenizer.cls_token_id = tk_tokenizer.token_to_id(\"<cls>\")\n",
    "tokenizer.sep_token = \"<sep>\"\n",
    "tokenizer.sep_token_id = tk_tokenizer.token_to_id(\"<sep>\")\n",
    "tokenizer.mask_token = \"<mask>\"\n",
    "tokenizer.mask_token_id = tk_tokenizer.token_to_id(\"<mask>\")\n",
    "# and save for later!\n",
    "tokenizer.save_pretrained(\"./tokenizer-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"./tokenizer-test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "tokenized_texts = [torch.tensor(tokenizer.encode(text, truncation=True, max_length=2048)) for text in dataset[\"text\"][:1000]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_batch(batch):\n",
    "    return pad_sequence(batch, padding_value=tokenizer.pad_token_id).long()\n",
    "\n",
    "dataloader = DataLoader(tokenized_texts, batch_size=32, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_batch(batch):\n",
    "    return pad_sequence(batch, padding_value=tokenizer.pad_token_id).long()\n",
    "\n",
    "dataloader = DataLoader(tokenized_texts, batch_size=32, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "model = LSTMTitanic(vocab_size, embed_size, hidden_size, num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        # Get the current batch size\n",
    "        current_batch_size = batch.size(0)\n",
    "\n",
    "        # Initialize or resize hidden state based on the current batch size\n",
    "        hidden = model.init_hidden(current_batch_size)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(batch, hidden)\n",
    "        loss = criterion(output.view(-1, vocab_size), batch.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def generate_text(model, start_str, length, temperature=1.0):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Tokenize and encode the start string\n",
    "    input_ids = torch.tensor(tokenizer.encode(start_str)).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden(1)  # Batch size is 1 for single sequence generation\n",
    "\n",
    "    # Generate text\n",
    "    model_output = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            output, hidden = model(input_ids, hidden)\n",
    "            # Apply temperature to the last token's logits and sample a token\n",
    "            token_logits = output[0, -1, :] / temperature\n",
    "            predicted_token_id = torch.multinomial(F.softmax(token_logits, dim=-1), num_samples=1).item()\n",
    "\n",
    "            # Append predicted token ID to model_output and input_ids\n",
    "            model_output.append(predicted_token_id)\n",
    "            input_ids = torch.cat([input_ids, torch.tensor([[predicted_token_id]])], dim=1)\n",
    "\n",
    "    # Decode the tokens and return the text\n",
    "    return tokenizer.decode(model_output)\n",
    "\n",
    "# Example usage\n",
    "start_str = \"The weather today\"\n",
    "generated_text = generate_text(model, start_str, length=50, temperature=0.7)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "silly_tavern",
   "language": "python",
   "name": "silly_tavern"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
