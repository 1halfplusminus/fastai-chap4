{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fastbook transformers tokenizers==0.12.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAP 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall protobuf==3.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastbook\n",
    "import torch\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "import numpy as np\n",
    "import numpy.random as nprnd\n",
    "\n",
    "fastbook.setup_book()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quad(a,b,c,x): return a*x**2 + b*x + c\n",
    "r = quad(3,2,1,1.5)\n",
    "print(r)\n",
    "\n",
    "def make_quad(a,b,c): return lambda x: a*x**2 + b*x + c\n",
    "q = make_quad(3,2,1)\n",
    "print(q(1.5))\n",
    "np.random.seed(42)\n",
    "def noise(x,scale):  return nprnd.normal(scale=scale,size=x.shape) \n",
    "def add_noise(x,mult,add): return x + mult*noise(x,add)\n",
    "\n",
    "x = torch.linspace(-2,2,steps=20)[:,None]\n",
    "y = add_noise(q(x),0.3,1.5)\n",
    "plt.scatter(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\"\"\" @interact(a=1.5,b=1.5,c=1.5) \"\"\"\n",
    "def plot_quad(a,b,c):\n",
    "    f = make_quad(a,b,c)\n",
    "    loss= mse(f(x),y)\n",
    "    plot_function(f,title=f'Loss: {loss}')\n",
    "    plt.scatter(x,y)\n",
    "\n",
    "def mse(preds,acts): return ((preds-acts)**2).mean().sqrt()\n",
    "def quad_mse(params):\n",
    "    f= make_quad(*params)\n",
    "    return mse(f(x),y)\n",
    "\n",
    "def rectified_linear(m,b,x):\n",
    "    y = m*x+b\n",
    "    return torch.clip(y,0,)\n",
    "def double_relu(m1,b1,m2,b2,x):\n",
    "    return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)\n",
    "abc = torch.tensor((1.5,1.5,1.5),requires_grad=True)\n",
    "loss = quad_mse(abc)\n",
    "print(f'Loss: {loss}')\n",
    "loss.backward()\n",
    "\n",
    "abc.grad\n",
    "with torch.no_grad():\n",
    "    abc -= abc.grad * 0.01\n",
    "    loss = quad_mse(abc)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "\n",
    "for i in range(20):\n",
    "    loss = quad_mse(abc)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        abc -= abc.grad * 0.01\n",
    "        abc.grad.zero_()\n",
    "    \"\"\" print(f'Loss: {loss}') \"\"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(m1=1.5,b1=1.5,m2=1.5,b2=1.5)\n",
    "def plot_rdouble_relu(m1,b1,m2,b2):\n",
    "    plot_function(partial(double_relu,m1,b1,m2,b2))\n",
    "    plt.scatter(x,y)\n",
    "# 3x3 matrix\n",
    "matrix1 = torch.tensor([[1, 2, 1],\n",
    "                        [0, 1, 0],\n",
    "                        [2, 3, 4]])\n",
    "\n",
    "# 3x2 matrix\n",
    "matrix2 = torch.tensor([[2, 5],\n",
    "                        [6, 7],\n",
    "                        [1, 8]])\n",
    "result = torch.matmul(matrix1, matrix2)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "df = pd.read_csv('./train.csv')\n",
    "scaler = MinMaxScaler()\n",
    "columns_to_normalize = ['Age', 'Fare','SibSp','Parch']\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "df['Fare'].fillna(df['Fare'].median(), inplace=True)\n",
    "df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])\n",
    "df_encoded = pd.get_dummies(df,columns=['Sex','Embarked','Pclass'])\n",
    "df_encoded.drop(['PassengerId','Name','Ticket','Cabin'],axis=1,inplace=True)\n",
    "# Convert boolean columns to integers\n",
    "bool_columns = df_encoded.select_dtypes(include='bool').columns\n",
    "df_encoded[bool_columns] = df_encoded[bool_columns].astype(int)\n",
    "\n",
    "# Split the data into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_encoded, df['Survived'], test_size=0.2)\n",
    "\n",
    "# Convert DataFrames to PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "model = SimpleNeuralNetwork(input_size=df_encoded.shape[1], hidden_size=16, output_size=1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "for epoch in range(200):\n",
    "    y_pred = model(X_train_tensor)\n",
    "\n",
    "    loss = torch.mean((y_pred.squeeze() - y_train_tensor) ** 2)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "y_pred = model(X_test_tensor)\n",
    "accuracy = ((y_pred > 0.5).squeeze().float() == y_test_tensor).float().mean()\n",
    "# Run the model on the entire test dataset\n",
    "y_pred_test = model(X_test_tensor)\n",
    "\n",
    "# Convert the predictions to binary labels using a threshold of 0.5\n",
    "y_pred_binary = (y_pred_test > 0.5).squeeze().float()\n",
    "\n",
    "# Convert the PyTorch tensor containing the predictions to a NumPy array\n",
    "y_pred_numpy = y_pred_binary.detach().numpy()\n",
    "\n",
    "# Create a new DataFrame with the desired columns\n",
    "predictions_df = pd.DataFrame(data={'Index': X_test.index, 'Prediction': y_pred_numpy})\n",
    "# Compare the predicted labels with the true labels and count the matches\n",
    "correct_predictions = (y_pred_binary == y_test_tensor).float().sum().item()\n",
    "\n",
    "print(f\"Number of correctly predicted samples: {correct_predictions}\\{y_test_tensor.shape[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAP 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "df = pd.read_csv('./train.csv')\n",
    "df.describe(include=[\"object\"])\n",
    "df['input'] = 'TEXT1:'+ df.context + '; TEXT2:'+df.target + '; ANC1: '+ df.anchor\n",
    "df.input.head()\n",
    "dataset = Dataset.from_pandas(df)\n",
    "model_nm = 'microsoft/deberta-v3-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_nm)\n",
    "tokenizer.tokenize(\"I love you. I hate you. I don't know.\")\n",
    "def tok_func(x): return tokenizer(x['input'])\n",
    "\n",
    "tok_ds = dataset.map(tok_func,batched=True)\n",
    "row = tok_ds[0]\n",
    "tok_ds = tok_ds.rename_columns({'score':'labels'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv('./test.csv')\n",
    "eval_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "housing : pd.DataFrame  = datasets.fetch_california_housing(as_frame=True) # type: ignore\n",
    "\n",
    "housing = housing[\"data\"].join(housing[\"target\"]).sample(1000,random_state=52)\n",
    "\n",
    "housing.head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2,suppress=True)\n",
    "\n",
    "def corr(x,y): return np.corrcoef(x,y)[0,1]\n",
    "def show_corr(df,a,b):\n",
    "    x,y = df[a],df[b]\n",
    "    plt.scatter(x,y,alpha=0.5,s=4)\n",
    "    plt.title(f'Correlation between {a} and {b}: {corr(x,y):.2f}')\n",
    "np.corrcoef(housing,rowvar=True)\n",
    "corr(housing.MedInc,housing.MedHouseVal)\n",
    "\n",
    "subset = housing[housing.AveRooms < 15]\n",
    "show_corr(subset,'MedInc','AveRooms')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_d(eval_pred): return {'pearson': corr(*eval_pred)}\n",
    "\n",
    "dds = tok_ds.train_test_split(0.25,seed=42)\n",
    "dds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments,Trainer\n",
    "\n",
    "bs = 128\n",
    "epochhs = 4\n",
    "lr = 8e-5\n",
    "args = TrainingArguments(\n",
    "    'ouputs',\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochhs,\n",
    "    weight_decay=0.01,\n",
    "    report_to=None,\n",
    "    learning_rate=lr,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm,num_labels=1)\n",
    "trainer = Trainer(model,args,train_dataset=dds['train'],eval_dataset=dds['test'],tokenizer=tokenizer,compute_metrics=corr_d) # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df['input'] = 'TEXT1:'+ eval_df.context + '; TEXT2:'+eval_df.target + '; ANC1: '+ eval_df.anchor\n",
    "eval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True)\n",
    "preds = trainer.predict(eval_ds) # type: ignore\n",
    "preds = preds.predictions.astype(float).clip(0,1)     # type: ignore\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "eval_df['input'] = 'TEXT1:'+ eval_df.context + '; TEXT2:'+eval_df.target + '; ANC1: '+ eval_df.anchor\n",
    "eval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True)\n",
    "preds = trainer.predict(eval_ds) # type: ignore\n",
    "preds = preds.predictions.astype(float).clip(0,1)     # type: ignore\n",
    "print(preds)\n",
    "# assuming preds is a numpy array, remove single-dimensional entries from the shape of the array\n",
    "simple_preds = preds.squeeze()\n",
    "\n",
    "# create the submission dataframe\n",
    "submission = pd.DataFrame({'id': eval_df['id'], 'score': simple_preds})\n",
    "\n",
    "# save to csv\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)\n",
    "pets = DataBlock(blocks=[ImageBlock, CategoryBlock],\n",
    "                 get_items=get_image_files,\n",
    "                 splitter=RandomSplitter(seed=42),\n",
    "                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n",
    "                 item_tfms=[RandomResizedCrop(460, min_scale=0.75)],\n",
    "                 batch_tfms=[*aug_transforms(size=224, max_warp=0), Normalize.from_stats(*imagenet_stats)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = pets.dataloaders(path/\"images\")\n",
    "learn = Learner(dls, resnet18, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n",
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the input data\n",
    "x = torch.tensor([[1.0], [2.0], [3.0], [4.0]], dtype=torch.float32)\n",
    "\n",
    "# Define the target data\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]], dtype=torch.float32)\n",
    "\n",
    "# Define the model\n",
    "model = torch.nn.Linear(1, 1)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model(x)\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
