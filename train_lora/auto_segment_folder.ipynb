{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio  xformers==0.0.20  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = \"/notebooks\"\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {HOME}\n",
    "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
    "%cd {HOME}/GroundingDINO\n",
    "%pip install -q -e .\n",
    "%pip install -q roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/facebookresearch/segment-anything.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python pycocotools matplotlib onnxruntime onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "WEIGHTS_NAME = \"groundingdino_swint_ogc.pth\"\n",
    "WEIGHTS_PATH = \"/notebooks/ComfyUI/models/grounding-dino/groundingdino_swint_ogc.pth\"\n",
    "print(WEIGHTS_PATH, \"; exist:\", os.path.isfile(WEIGHTS_PATH))\n",
    "CONFIG_PATH = os.path.join(HOME, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
    "print(CONFIG_PATH, \"; exist:\", os.path.isfile(CONFIG_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input and output directories\n",
    "input_dir = '/notebooks/lora_training/Blockout/blockout/to_crop'\n",
    "output_dir = '/notebooks/lora_training/Blockout/blockout/cropped'\n",
    "\n",
    "# Set device to cpu for Mac M1\n",
    "device = \"cuda\"\n",
    "# Load the SAM model\n",
    "sam_checkpoint = \"/notebooks/ComfyUI/models/sams/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "\n",
    "model = load_model(CONFIG_PATH, WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_PROMPT = \"character\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        continue  # Skip to the next iteration if the file is a .txt file\n",
    "    # Load the image using Pillow\n",
    "    image_path = os.path.join(input_dir, filename)\n",
    "    try:\n",
    "        image_pil = Image.open(image_path)\n",
    "    except IOError:\n",
    "        print(f\"Cannot open {filename}. Skipping...\")\n",
    "        continue \n",
    "\n",
    "    # Convert the Pillow Image object to a NumPy array (as the mask generator expects a NumPy array)\n",
    "    image_np = np.array(image_pil)\n",
    "    \n",
    "    # Generate masks for the image\n",
    "    image_source, image = load_image(image_path)\n",
    "\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=model, \n",
    "        image=image, \n",
    "        caption=TEXT_PROMPT, \n",
    "        box_threshold=BOX_TRESHOLD, \n",
    "        text_threshold=TEXT_TRESHOLD\n",
    "    )\n",
    "\n",
    "  \n",
    "    image_width, image_height = image_pil.size\n",
    "    # Convert the normalized cxcywh coordinates to pixel xyxy coordinates\n",
    "    boxes_pixel_coords = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\") * torch.Tensor([image_width, image_height, image_width, image_height])\n",
    "\n",
    "    for i, box in enumerate(boxes_pixel_coords):\n",
    "        # The box_convert function returns a tensor with the same dtype as the input.\n",
    "        # Convert the tensor to a numpy array and then to int for use with Image.crop.\n",
    "        box = box.numpy().astype(int)\n",
    "\n",
    "        # Unpack the box coordinates\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "          # Convert the normalized bounding box coordinates to pixel coordinates\n",
    "        cropped_image_pil = image_pil.crop((x_min, y_min, x_max, y_max))\n",
    "        output_path = os.path.join(output_dir, f'{filename.split(\".\")[0]}_crop{i}.png')\n",
    "        cropped_image_pil.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm /notebooks/lora_training/Blockout/blockout/cropped/* -rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Real-ESRGAN and enter the Real-ESRGAN\n",
    "!git clone https://github.com/xinntao/Real-ESRGAN.git\n",
    "%cd Real-ESRGAN\n",
    "# Set up the environment\n",
    "%pip install basicsr\n",
    "%pip install facexlib\n",
    "%pip install gfpgan\n",
    "%pip install -r requirements.txt\n",
    "%run setup.py develop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "louis",
   "language": "python",
   "name": "louis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
