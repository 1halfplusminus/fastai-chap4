{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install termcolor  langchain_experimental tiktoken sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/facebookresearch/faiss.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get -y install libblas-dev liblapack-dev swig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf build/\n",
    "#!mkdir build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd /notebooks/faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd /notebooks/faiss\n",
    "#!cmake  -B build ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!make -C build -j faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!make -C build -j swigfaiss\n",
    "##%cd build/faiss/python\n",
    "#%run setup.py install\n",
    "#%cd /notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv /notebooks/chainlang/my.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "from termcolor import colored\n",
    "\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "LLM=LlamaCpp(\n",
    "    model_path=\"/notebooks/zephyr-7b-alpha.Q8_0.gguf\",\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=False,\n",
    "    n_ctx=12000,\n",
    "    max_tokens=4500,\n",
    "    n_gpu_layers=65\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "LLM_MEMORY=LlamaCpp(\n",
    "    model_path=\"/notebooks/zephyr-7b-alpha.Q8_0.gguf\",\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=False,\n",
    "    callback_manager=callback_manager,\n",
    "    n_ctx=2800,\n",
    "    max_tokens=4500,\n",
    "    n_gpu_layers=34,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context='''\n",
    "Here the context for the characters personalities and story:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import ConversationChain  \n",
    "from langchain.memory import ConversationSummaryMemory, CombinedMemory,VectorStoreRetrieverMemory\n",
    "\n",
    "template=f'''\n",
    "Welcome to the Interactive Chat System! Here, you can engage in a guided story with our AI friend.\n",
    "The goal of the system is to help writing an story. \n",
    "The user input dialogue in a format or request descriptions of scenes and actions.\n",
    "The system will also respond or react as  character's specified by [AI]CHARACTER NAME[/AI] based on described human actions. Let's begin!\n",
    "[EXAMPLE]\n",
    "    Human:\n",
    "    [DIALOG] A to B: Hey, how are you? [/DIALOG]\n",
    "    [AI]B[/AI]    \n",
    "    [DIALOG] A to B: Hihihi *giggle* Fine and you ? [/DIALOG]\n",
    "    Human:\n",
    "    [ACTION] Look at A [/ACTION]\n",
    "    [DIALOG]: A to B: Wanna go grab something to eat ? [/DIALOG]\n",
    "    [AI] B  [/AI]     \n",
    "\n",
    "    [ACTION] Look around while responding[/ACTION]\n",
    "    [DIALOG] A to B: yep let's go [/DIALOG]\n",
    "    Human:\n",
    "    [DESCRIPTIONS] Describe the action[/DESCRIPTIONS]\n",
    "    [AI] B  [/AI]      \n",
    "  \n",
    "    [DESCRIPTIONS] On a calm and uneventful Saturday morning, A found himself standing before his neighbor's plain, white door. The street was quiet, the usual bustle of the neighborhood had not yet begun. The sky overhead was a clear blue, with the sun casting a warm, gentle light on the houses lining the street. The birds chirped cheerily, oblivious to the human dramas unfolding below them.\n",
    "\n",
    "A was about to knock on the door when a sudden shout broke the tranquility of the morning [/DESCRIPTIONS]   \n",
    "[/EXAMPLE]\n",
    "'''\n",
    "memory_template = \"\"\"\n",
    "[CONTEXT]\n",
    "{context}\n",
    "[/CONTEXT]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message_prompt,\n",
    "    SystemMessagePromptTemplate.from_template(memory_template),\n",
    "    AIMessagePromptTemplate.from_template(\"Alright, let's get started...\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_summary\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    human_message_prompt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import ConversationChain  \n",
    "from langchain.memory import ConversationSummaryMemory, CombinedMemory,VectorStoreRetrieverMemory\n",
    "\n",
    "template=f'''\n",
    " \n",
    "'''\n",
    "memory_template = \"\"\"\n",
    "[CONTEXT]\n",
    "{context}\n",
    "[/CONTEXT]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message_prompt,\n",
    "    SystemMessagePromptTemplate.from_template(memory_template),\n",
    "    AIMessagePromptTemplate.from_template(\"Alright, let's get started...\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_summary\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    human_message_prompt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.generative_agents import (\n",
    "    GenerativeAgent,\n",
    "    GenerativeAgentMemory,\n",
    ")\n",
    "import math\n",
    "import faiss\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.memory import ConversationSummaryMemory, VectorStoreRetrieverMemory\n",
    "\n",
    "def relevance_score_fn(score: float) -> float:\n",
    "    \"\"\"Return a similarity score on a scale [0, 1].\"\"\"\n",
    "    # This will differ depending on a few things:\n",
    "    # - the distance / similarity metric used by the VectorStore\n",
    "    # - the scale of your embeddings (OpenAI's are unit norm. Many others are not!)\n",
    "    # This function converts the euclidean norm of normalized embeddings\n",
    "    # (0 is most similar, sqrt(2) most dissimilar)\n",
    "    # to a similarity function (0 to 1)\n",
    "    return score\n",
    "\n",
    "\n",
    "def create_new_memory_retriever():\n",
    "    \"\"\"Create a new vector store retriever unique to the agent.\"\"\"\n",
    "    # Initialize the vectorstore as empty\n",
    "    embedding =  HuggingFaceEmbeddings(\n",
    "            model_name=\"intfloat/multilingual-e5-large\"\n",
    "    )\n",
    "    embedding_size = 1024\n",
    "    index = faiss.IndexFlatL2(embedding_size)\n",
    "    vectorstore = FAISS(embedding, index, InMemoryDocstore({}), {})\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"mmr\", # Also test \"similarity\"\n",
    "        search_kwargs={\"k\": 8},\n",
    "    )\n",
    "    return VectorStoreRetrieverMemory(retriever=retriever,input_key=\"text\",memory_key=\"chat_memory\",return_messages=False,output_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_memory = create_new_memory_retriever()\n",
    "current_chat_memory = ConversationBufferMemory(memory_key=\"chat_history\",return_messages=True,input_key=\"text\",output_key=\"text\",max_history=4)\n",
    "summary_memory = ConversationSummaryMemory(llm=LLM_MEMORY,memory_key=\"chat_summary\",return_messages=True,input_key=\"text\")\n",
    "memory = CombinedMemory(memories=[current_chat_memory, summary_memory])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation = LLMChain(\n",
    "    llm=LLM,\n",
    "    prompt=chat_prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation({\"text\": '''\n",
    "[INSTRUCTION]\n",
    "[/INSTRUCTION]\n",
    "[AI][/AI]\n",
    "''',\"context\":context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_chat_memory.chat_memory.messages.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_chat_memory.chat_memory.messages.pop(len(current_chat_memory.chat_memory.messages)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_chat_memory.clear()\n",
    "summary_memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "import os\n",
    "def reset_all_memory():\n",
    "    current_chat_memory.clear()\n",
    "    summary_memory.clear()\n",
    "    summary_memory.clear()\n",
    "def reset_conversation():\n",
    "    current_chat_memory.clear()\n",
    "def forget_last_message():\n",
    "    current_chat_memory.chat_memory.messages.pop(0)\n",
    "def predict(message, history,context, actions, descriptions, ai):\n",
    "    # Initialize an empty list to collect the formatted sections\n",
    "    formatted_sections = []\n",
    "\n",
    "    # Check each variable and append its formatted section to formatted_sections\n",
    "    if message:\n",
    "        formatted_sections.append(f'[DIALOG]\\n{message}\\n[/DIALOG]')\n",
    "    if actions:\n",
    "        formatted_sections.append(f'[ACTION]\\n{actions}\\n[/ACTION]')\n",
    "    if descriptions:\n",
    "        formatted_sections.append(f'[DESCRIPTION]\\n{descriptions}\\n[/DESCRIPTION')\n",
    "    if ai:\n",
    "        formatted_sections.append(f'[AI]\\n{ai}\\n[/AI]')\n",
    "\n",
    "    # Join the formatted sections with newline characters to create formatted_input\n",
    "    formatted_input = '\\n'.join(formatted_sections)\n",
    "\n",
    "    # The rest of your code remains the same\n",
    "    history_langchain_format = []\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "    history_langchain_format.append(HumanMessage(content=message))\n",
    "\n",
    "    # Getting the response from langchain\n",
    "    gpt_response = conversation({\"text\": formatted_input,\"context\":context})\n",
    "    for message in gpt_response[\"chat_history\"]:\n",
    "        history_langchain_format.append(message)\n",
    "    print(gpt_response)\n",
    "    return gpt_response[\"text\"]\n",
    "\n",
    "# Defining additional input fields\n",
    "additional_inputs = [\n",
    "    gr.Textbox(default=\"Enter actions here\", label=\"Actions\",lines=2),\n",
    "    gr.Textbox(default=\"Enter descriptions here\", label=\"Descriptions\",lines=2),\n",
    "    gr.Textbox(default=\"Enter AI input here\", label=\"AI\",lines=2),\n",
    "    gr.Textbox(default=\"Context\", label=\"Context\",lines=5,value=context),\n",
    "    gr.Button(label=\"Forget last message\", on_click=forget_last_message),\n",
    "    gr.Button(label=\"Reset Conversation\", on_click=reset_conversation),\n",
    "    gr.Button(label=\"Reset all memory\", on_click=reset_all_memory),\n",
    "    \n",
    "]\n",
    "\n",
    "# Creating the ChatInterface with additional input fields\n",
    "chat_interface = gr.ChatInterface(\n",
    "    fn=predict, \n",
    "    additional_inputs=additional_inputs\n",
    ")\n",
    "\n",
    "chat_interface.launch(share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tommies_memory = GenerativeAgentMemory(\n",
    "    llm=LLM,\n",
    "    memory_retriever=create_new_memory_retriever(),\n",
    "    verbose=False,\n",
    "    reflection_threshold=12,  # we will give this a relatively low number to show how reflection works\n",
    ")\n",
    "\n",
    "tommie = GenerativeAgent(\n",
    "    name=\"Minoru\",\n",
    "    age=20,\n",
    "    traits=\"anxious, likes design, talkative\",  # You can add more persistent traits here\n",
    "    status=\"looking for job\",  # When connected to a virtual world, we can have the characters update their status\n",
    "    memory_retriever=create_new_memory_retriever(),\n",
    "    llm=LLM,\n",
    "    memory=tommies_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tommie.get_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tommie.get_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.utils import filter_complex_metadata\n",
    "# We can add memories directly to the memory object\n",
    "tommie_observations = [\n",
    "\n",
    "]\n",
    "for observation in tommie_observations:\n",
    "    tommie.memory.add_memory(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = [\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's send Tommie on their way. We'll check in on their summary every few observations to watch it evolve\n",
    "for i, observation in enumerate(observations):\n",
    "    _, reaction = tommie.generate_reaction(observation)\n",
    "    print(colored(observation, \"green\"), reaction)\n",
    "    if ((i + 1) % 20) == 0:\n",
    "        print(\"*\" * 40)\n",
    "        print(\n",
    "            colored(\n",
    "                f\"After {i+1} observations, Minoro's summary is:\\n{tommie.get_summary(force_refresh=True)}\",\n",
    "                \"blue\",\n",
    "            )\n",
    "        )\n",
    "        print(\"*\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
