{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U insightface onnxruntime-gpu  numpy fastai pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U gdown moviepy ffmpeg insightface natsort hdbscan imutils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import cv2\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip,AudioFileClip\n",
    "import glob\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "from insightface.data import get_image as ins_get_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define variables\n",
    "drive_file_id = '1kz9-3Q-pn0VX5E8-0VWoJsNNwhTzZwCj'  # replace 'FILE_ID' with your file's ID\n",
    "video_output = 'my_video.mp4'\n",
    "frame_folder = '/notebooks/frame_folder'\n",
    "swapped_folder = '/notebooks/swapped'\n",
    "character_folder = '/notebooks/character'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(frame_folder)\n",
    "shutil.rmtree(swapped_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Resize transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Extract embeddings and images\n",
    "embeddings = [face['face_embedding'] for face in face_data]\n",
    "images = [cv2.cvtColor(face['image'], cv2.COLOR_BGR2RGB) for face in face_data]  # Convert from BGR to RGB\n",
    "images = [transform(image) for image in images]  # Apply the transformation\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "embeddings = torch.stack([torch.Tensor(e) for e in embeddings])\n",
    "images = torch.stack(images)\n",
    "\n",
    "# Create a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, embeddings, images):\n",
    "        self.embeddings = embeddings\n",
    "        self.images = images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.images[idx]\n",
    "\n",
    "# Create a DataLoader\n",
    "dataset = CustomDataset(embeddings, images)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultimodalAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder for embeddings\n",
    "        self.embedding_encoder = nn.Sequential(\n",
    "            nn.Linear(embeddings.shape[1], 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True))\n",
    "        \n",
    "        # Encoder for images\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=5),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*13*13, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "        # Decoder for embeddings\n",
    "        self.embedding_decoder = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, embeddings.shape[1]),\n",
    "            nn.ReLU(True))\n",
    "        \n",
    "        # Decoder for images\n",
    "        self.image_decoder = nn.Sequential(\n",
    "            nn.Linear(64, 128*8*8),\n",
    "            nn.ReLU(True),\n",
    "            nn.Unflatten(1, (128, 8, 8)),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # Added upsampling layer\n",
    "            nn.Conv2d(32, 3, kernel_size=1))\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.embedding_encoder(x1)\n",
    "        x2 = self.image_encoder(x2)\n",
    "        x1 = self.embedding_decoder(x1)\n",
    "        x2 = self.image_decoder(x2)\n",
    "        return x1, x2\n",
    "\n",
    "# Initialize the autoencoder and optimizer\n",
    "model = MultimodalAutoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "device = \"cuda\"\n",
    "model = model.to(device)\n",
    "# Train the autoencoder\n",
    "num_epochs = 1000\n",
    "print(embeddings.shape)  # print shape of original embeddings\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for embeddings, images in dataloader:\n",
    "        embeddings = embeddings.to(device)\n",
    "        images = images.to(device)\n",
    "        embeddings_output, images_output = model(embeddings, images)\n",
    "        loss_embeddings = criterion(embeddings_output, embeddings)\n",
    "        loss_images = criterion(images_output, images)\n",
    "        loss = loss_embeddings + loss_images\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "# Load model\n",
    "model = MultimodalAutoencoder()\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.stack([torch.Tensor(e) for e in embeddings])\n",
    "model = model.to(device)\n",
    "embeddings.to(device)\n",
    "embeddings = model.embedding_encoder(embeddings)\n",
    "reconstructed_images = model.image_decoder(embeddings)\n",
    "reconstructed_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# assuming reconstructed_images is a tensor\n",
    "reconstructed_images_np = reconstructed_images.cpu().detach().numpy()\n",
    "reconstructed_images_np = np.transpose(reconstructed_images_np, (0, 2, 3, 1))  # convert from NCHW to NHWC\n",
    "reconstructed_image = Image.fromarray(np.uint8(reconstructed_images_np[3] * 255))\n",
    "reconstructed_image.show()\n",
    "\n",
    "faces = app.get(np.uint8(reconstructed_images_np[0] * 255))\n",
    "print(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert insightface.__version__ >= '0.7'\n",
    "\n",
    "\n",
    "# Create the frame folder if it doesn't exist\n",
    "if not os.path.exists(frame_folder):\n",
    "    os.makedirs(frame_folder)\n",
    "# Create the frame folder if it doesn't exist\n",
    "if not os.path.exists(swapped_folder):\n",
    "    os.makedirs(swapped_folder)\n",
    "# Your face swapping script\n",
    "app = FaceAnalysis()\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "swapper = insightface.model_zoo.get_model('/notebooks/inswapper_128.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_first_face(image):\n",
    "    # If the image is a string (presumably a file path), read the image\n",
    "    if isinstance(image, str):\n",
    "        image = cv2.imread(image)\n",
    "\n",
    "    # Check if the image is a valid numpy array\n",
    "    if isinstance(image, np.ndarray):\n",
    "        source_faces = app.get(image)\n",
    "        source_faces = sorted(source_faces, key=lambda x: x.bbox[0])\n",
    "        if len(source_faces) == 0:\n",
    "            print(image)\n",
    "            assert False\n",
    "\n",
    "        return source_faces[0]\n",
    "    print(image)\n",
    "    assert False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://example.com/path_to_your_file\"  # replace with your file's URL\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(video_output, 'wb') as f:  # replace with the path where you want to save the file\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download video from Google Drive\n",
    "url = f'https://drive.google.com/uc?id={drive_file_id}'\n",
    "gdown.download(url, video_output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete if swapped folder is not empty\n",
    "if  os.path.exists(character_folder):\n",
    "    shutil.rmtree(character_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "import datetime\n",
    "# Create the frame folder if it doesn't exist\n",
    "if not os.path.exists(character_folder):\n",
    "    os.makedirs(character_folder)\n",
    "if not os.path.exists(frame_folder):\n",
    "    os.makedirs(frame_folder)\n",
    "# Initialize the FaceAnalysis application\n",
    "\"\"\" app = FaceAnalysis(providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))  # Use GPU device 0 and input image size as (640, 640) \"\"\"\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_output)\n",
    "# Calculate frame rate (frames per second)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "face_index= 0\n",
    "# Initialize a list to store embeddings and face data for each detected face\n",
    "face_data = []\n",
    "count = 0\n",
    "# Loop through the video file frame by frame\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        # Calculate the time in minutes and seconds\n",
    "    time_in_milliseconds = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "    frame_time = datetime.timedelta(milliseconds=time_in_milliseconds)\n",
    "    path = os.path.join(frame_folder, f\"{count}.jpg\")\n",
    "    cv2.imwrite(path, frame)  # save frame as JPEG file\n",
    "    # Use the FaceAnalysis application to detect faces in the frame\n",
    "    faces = app.get(frame)\n",
    "\n",
    "    # For each detected face, extract the embedding, bounding box, and face image, and add them to the list\n",
    "    for face in faces:\n",
    "        face_embedding = face.embedding\n",
    "        bbox = face.bbox.astype(int)\n",
    "        cropped_face = frame[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "        if cropped_face.size > 0:\n",
    "            face_data.append({'index':face_index,'label':'','frame':count,'time':frame_time,'face_embedding': face_embedding,'score':face.det_score,'normed_embedding': face.normed_embedding, 'bbox': bbox, 'image': cropped_face})\n",
    "        face_index += 1\n",
    "    \n",
    "    count += 1\n",
    "\n",
    "# Save the face_data to disk\n",
    "with open('face_data.pkl', 'wb') as f:\n",
    "    pickle.dump(face_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_similar(face1, face2, threshold=0.5):\n",
    "    # Extract normalized embeddings\n",
    "    embedding1 = face1.normed_embedding\n",
    "    embedding2 = face2.normed_embedding\n",
    "\n",
    "    # Compute the dot product (cosine similarity, because embeddings are normalized)\n",
    "    similarity = np.dot(embedding1, embedding2)\n",
    "\n",
    "    # Return True if the similarity is above the threshold, False otherwise\n",
    "    return similarity > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "class FaceDataManager:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        with open(self.file_path, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "\n",
    "    def save_data(self):\n",
    "        with open(self.file_path, 'wb') as f:\n",
    "            pickle.dump(self.data, f)\n",
    "\n",
    "    def update_label(self, frame_number, face_image, new_label,save= True):\n",
    "        for f in self.data:\n",
    "            if f['frame'] == frame_number and np.array_equal(f['image'], face_image):\n",
    "                f['label'] = new_label\n",
    "        if save:\n",
    "            self.save_data()\n",
    "    def update_labels(self,old_label ,new_label):\n",
    "        for f in self.data:\n",
    "            if f['label'] == old_label:\n",
    "                f['label'] = new_label\n",
    "        self.save_data()\n",
    "    \n",
    "global face_data_manager\n",
    "face_data_manager = FaceDataManager('face_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Delete if swapped folder is not empty\n",
    "if  os.path.exists(character_folder):\n",
    "    shutil.rmtree(character_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.csgraph import connected_components\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "from imutils import build_montages\n",
    "from sklearn.cluster import DBSCAN\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.manifold import MDS\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# calculate similarity matrix\n",
    "feats = []\n",
    "for face in face_data_manager.data:\n",
    "    feats.append(face[\"normed_embedding\"])\n",
    "feats = np.array(feats, dtype=np.float32)\n",
    "sims = np.dot(feats, feats.T)\n",
    "\"\"\" \n",
    "# define the similarity threshold\n",
    "threshold = 0.7  # set your value\n",
    "\n",
    "# create an adjacency matrix\n",
    "adjacency = sims > threshold\n",
    "\n",
    "# find connected components\n",
    "n_components, labels = connected_components(csgraph=csr_matrix(adjacency), directed=False)\n",
    "print(labels)\n",
    "# print the number of groups\n",
    "print(f'Total groups: {n_components}')\n",
    "uniqueLabels = np.unique(labels)\n",
    "print(uniqueLabels)\n",
    "for label in uniqueLabels:\n",
    "    # Skip the noise\n",
    "    if label == -1:\n",
    "        continue  \n",
    "\n",
    "    directory_path = os.path.join(character_folder, str(label))\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "    # Select samples associated with the current label\n",
    "    idxs = np.where(labels == label)[0]\n",
    "    faces = []\n",
    "    # loop over the sampled indexes\n",
    "    for i in idxs:\n",
    "        # Get the cropped face image from the 'image' key\n",
    "        face = face_data_manager.data[i][\"image\"]\n",
    "        frame = face_data_manager.data[i][\"frame\"]\n",
    "        face_data_manager.data[i]['label'] = str(label)\n",
    "        time_elapsed= face_data_manager.data[i]['time']\n",
    "        # Calculate minutes and seconds\n",
    "        minutes = time_elapsed.total_seconds() // 60\n",
    "        seconds = time_elapsed.total_seconds() % 60\n",
    "\n",
    "        # Format the time string as MM:SS\n",
    "        frame_time = \"{:02}:{:02}\".format(int(minutes), int(seconds))\n",
    "        # Force resize the face to 96x96 and then add it to the\n",
    "        # faces montage list\n",
    "        face = cv2.resize(face, (150, 150))\n",
    "        cv2.putText(face,frame_time, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "        faces.append(face)\n",
    "\n",
    "        # Save face image\n",
    "        #image_path = os.path.join(directory_path, f\"{i}.jpg\")\n",
    "        #cv2.imwrite(image_path, face)\n",
    "\n",
    "    # Create a montage using 96x96 \"tiles\" with 5 rows and 5 columns\n",
    "    montage = build_montages(faces, (150, 150), (10, 8))[0]\n",
    "    # Save the output montage\n",
    "    title = \"Face ID #{}\".format(label)\n",
    "    title = \"Unknown Faces\" if label == -1 else title\n",
    "    cv2.imwrite(os.path.join(directory_path, title+'.jpg'), montage)\n",
    "\n",
    "    \n",
    "face_data_manager.save_data() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "from imutils import build_montages\n",
    "from sklearn.cluster import DBSCAN\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# Check if the directories exist\n",
    "if not os.path.exists(character_folder):\n",
    "    os.makedirs(character_folder)\n",
    "\n",
    "\n",
    "\"\"\" times = np.array([d['time'].total_seconds() for d in face_data_manager.data])\n",
    "times = times / np.max(times)\n",
    "scores \"\"\"\n",
    "embeddings = [d['normed_embedding'] for d in face_data_manager.data]\n",
    "embeddings = normalize(embeddings, norm='l2')\n",
    "\n",
    "clustering = DBSCAN(eps=0.5, min_samples=5,metric=\"cosine\")\n",
    "clustering.fit(embeddings)\n",
    "\n",
    "labels = np.unique(clustering.labels_)\n",
    "print(labels)\n",
    "for label in clustering.labels_:\n",
    "    # Skip the noise\n",
    "    if label == -1:\n",
    "        continue  \n",
    "\n",
    "    directory_path = os.path.join(character_folder, str(label))\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "    # Select samples associated with the current label\n",
    "    idxs = np.where(clustering.labels_ == label)[0]\n",
    "    faces = []\n",
    "    # loop over the sampled indexes\n",
    "    for i in idxs:\n",
    "        face_data_manager.data[i]['label'] = str(label)\n",
    "    idxs = np.random.choice(idxs, size=min(25, len(idxs)), replace=False)\n",
    "    for i in idxs:\n",
    "        # Get the time elapsed\n",
    "        time_elapsed = face_data_manager.data[i]['time']\n",
    "        minutes = time_elapsed.total_seconds() // 60\n",
    "        seconds = time_elapsed.total_seconds() % 60\n",
    "        # Calculate minutes and seconds\n",
    "        minutes = time_elapsed.total_seconds() // 60\n",
    "        seconds = time_elapsed.total_seconds() % 60\n",
    "        # Get the face from the data\n",
    "        face = face_data_manager.data[i][\"image\"]\n",
    "        # Format the time string as MM:SS\n",
    "        frame_time = \"{:02}:{:02}\".format(int(minutes), int(seconds))\n",
    "        # Force resize the face to 96x96 and then add it to the\n",
    "        # faces montage list\n",
    "        face = cv2.resize(face, (150, 150))\n",
    "        cv2.putText(face,frame_time, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "        faces.append(face)\n",
    "    montage = build_montages(faces, (150, 150), (5, 5))[0]\n",
    "    # Save the output montage\n",
    "    title = \"Face ID #{}\".format(label)\n",
    "    title = \"Unknown Faces\" if label == -1 else title\n",
    "    cv2.imwrite(os.path.join(directory_path, title+'.jpg'), montage)\n",
    "    \n",
    "face_data_manager.save_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from fastai.vision.all import *\n",
    "from pathlib import Path\n",
    "from PIL import Image as PilImage\n",
    "import shutil\n",
    "\n",
    "path = Path(character_folder)\n",
    "labels = [data[\"label\"] for data in face_data_manager.data if data[\"label\"].isnumeric()]\n",
    "\n",
    "def rename_folder(old_name, new_name):\n",
    "    if new_name.exists():  # If the target directory exists\n",
    "     \n",
    "        shutil.rmtree(old_name)\n",
    "    else:\n",
    "        old_name.rename(new_name)\n",
    "        \n",
    "def create_on_button_clicked_handler(label, name):\n",
    "    global face_data_manager\n",
    "    def on_button_clicked(b):\n",
    "        global face_data_manager\n",
    "        new_label = name.value\n",
    "        print(f'Character name confirmed: {new_label}')\n",
    "\n",
    "        face_data_manager.update_labels(label, new_label)\n",
    "\n",
    "        old_name = Path(Path(path) / label)\n",
    "        new_name = Path(Path(path) / new_label)\n",
    "\n",
    "        rename_folder(old_name, new_name)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        next(process)\n",
    "\n",
    "    return on_button_clicked\n",
    "def process_folders(labels, path):\n",
    "    global face_data_manager\n",
    "    for label in labels:\n",
    "        image_paths = list((path / label).glob('*.jpg'))\n",
    "        if not image_paths:\n",
    "            continue\n",
    "        print(f\"Current cluster: {label}\")\n",
    "\n",
    "        image_resolution = {}\n",
    "        for image_path in image_paths:\n",
    "            with PilImage.open(image_path) as img:\n",
    "                width, height = img.size\n",
    "                resolution = width * height  # resolution is defined as width * height\n",
    "                image_resolution[image_path] = resolution\n",
    "\n",
    "        sorted_image_paths = sorted(image_resolution, key=image_resolution.get, reverse=True)\n",
    "        name = widgets.Text(value=label, placeholder='Enter character name', description='Name:')\n",
    "        button = widgets.Button(description='Confirm')\n",
    "        display(name, button)\n",
    "        with open(sorted_image_paths[0], \"rb\") as file:\n",
    "            image = file.read()\n",
    "            img_widget = widgets.Image(value=image, format='png', width=1000, height=400)\n",
    "            display(img_widget)\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "        button.on_click(create_on_button_clicked_handler(label, name))\n",
    "  \n",
    "        yield\n",
    "\n",
    "process = process_folders(labels, path)\n",
    "next(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in face_data_manager.data:\n",
    "    if data[\"label\"].isnumeric():\n",
    "        print(data[\"index\"],\"is numeric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from ipywidgets import HBox, VBox\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from fastai.vision.all import *\n",
    "from pathlib import Path\n",
    "from PIL import Image as PilImage\n",
    "import shutil\n",
    "from ipywidgets import GridBox\n",
    "\n",
    "path = Path(character_folder)\n",
    "processed_images = glob.glob(os.path.join(frame_folder, '*.jpg'))\n",
    "processed_images.sort(key=lambda f: int(re.sub('\\D', '', os.path.basename(f))))\n",
    "def on_button_clicked_factory(face, frame_number, name, path, character_folder):\n",
    "    global face_data_manager\n",
    "    def on_button_clicked(b):\n",
    "        global face_data_manager\n",
    "        label = name.value\n",
    "        face_data_manager.update_label(frame_number, face['image'], label)\n",
    "        \"\"\"  old_name = Path(path / str(face['label']))\n",
    "            new_name = Path(path / label)\n",
    "            if new_name.exists():  \n",
    "                # Copy all .jpg files in the current folder to the target folder\n",
    "                for file in old_name.glob('*.jpg'):\n",
    "                    if new_name != old_name:\n",
    "                        shutil.copy(file, new_name / file.name)\n",
    "                    # Copy all .npy files (embeddings) in the current folder to the target folder\n",
    "                for file in old_name.glob('*.npy'):\n",
    "                    if new_name != old_name:\n",
    "                        shutil.copy(file, new_name / file.name) \"\"\"\n",
    "\n",
    "\n",
    "        \"\"\"  else:\n",
    "                if not os.path.exists(new_name):\n",
    "                    os.mkdir(new_name)\n",
    "                old_name.rename(new_name) \"\"\"\n",
    "        \"\"\"     process_frame(first_frame_with_label_0) \"\"\"\n",
    "        # At the start of your code, add the following line to get the first frame that has a label '0'\n",
    "    \"\"\"     first_frame_with_label_0 = min([f['frame'] for f in face_data_manager.data if f['label'] == '0' or f['label'] == ''], default=0)\n",
    "  \n",
    "        slider.value = first_frame_with_label_0 \"\"\"\n",
    "    return on_button_clicked\n",
    "\n",
    "def process_frame(frame_number):\n",
    "    global face_data_manager\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "\n",
    "    frame_path = processed_images[frame_number]\n",
    "    frame = cv2.imread(str(frame_path))\n",
    "    \n",
    "\n",
    "    frame_widget = convert_frame_to_widget(frame)\n",
    "\n",
    "    frame_output.clear_output()\n",
    "\n",
    "    with frame_output:\n",
    "        print(f\"Frame number: {frame_number}\")\n",
    "\n",
    "        faces = [p for p in face_data_manager.data if p[\"frame\"] == int(frame_number)]\n",
    "        face_widgets = process_faces(faces,frame_number)\n",
    "\n",
    "        display(HBox([frame_widget] + face_widgets))\n",
    "        labels = set(face['label'] for face in face_data_manager.data)\n",
    "        image_boxes = [display_images(label) for label in labels]\n",
    "\n",
    "        grid = widgets.GridBox(image_boxes, layout=widgets.Layout(grid_template_columns=\"repeat(4, 250px)\"))\n",
    "        display(grid)\n",
    "\n",
    "def convert_frame_to_widget(frame):\n",
    "    frame_pil = PilImage.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    base_width = 500\n",
    "    w_percent = (base_width / float(frame_pil.size[0]))\n",
    "    h_size = int((float(frame_pil.size[1]) * float(w_percent)))\n",
    "    frame_pil = frame_pil.resize((base_width, h_size))\n",
    "\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    frame_pil.save(img_byte_arr, format='PNG')\n",
    "    return widgets.Image(value=img_byte_arr.getvalue(), format='png')\n",
    "\n",
    "\n",
    "def process_faces(faces, frame_number):\n",
    "    face_widgets = []\n",
    "    for face in faces:\n",
    "        print(f\"Time: {face['time']}\")\n",
    "        cropped_face_widget = convert_face_to_widget(face['image'])\n",
    "        name = widgets.Text(value=str(face['label']), placeholder='Enter character name', description='Name:')\n",
    "        button = widgets.Button(description='Confirm')\n",
    "\n",
    "        on_button_clicked = on_button_clicked_factory(face, frame_number, name, path, character_folder)\n",
    "        button.on_click(on_button_clicked)\n",
    "        face_box = VBox([cropped_face_widget, name, button])\n",
    "        face_widgets.append(face_box)\n",
    "    return face_widgets\n",
    "\n",
    "\n",
    "def convert_face_to_widget(face):\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)  # Add this line to convert the color space\n",
    "    cropped_face_pil = PilImage.fromarray(face)\n",
    "    cropped_face_pil = cropped_face_pil.resize((200, 200))\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    cropped_face_pil.save(img_byte_arr, format='PNG')\n",
    "    return widgets.Image(value=img_byte_arr.getvalue(), format='jpg')\n",
    "def calculate_centroid(embeddings):\n",
    "    return np.mean(embeddings, axis=0)\n",
    "def display_images(label):\n",
    "    # Find the central image for this label\n",
    "    central_image = find_central_images(label)\n",
    "\n",
    "    # Convert the image to a widget\n",
    "    central_image_widget = convert_face_to_widget(central_image)\n",
    "\n",
    "    # Return a box containing the image widget\n",
    "    return VBox([widgets.Label(f\"Label: {label}\"), central_image_widget])\n",
    "def find_central_images(label):\n",
    "    # Gather all embeddings for this label\n",
    "    label_embeddings = [d['normed_embedding'] for d in face_data_manager.data if d['label'] == str(label)]\n",
    "    # Calculate the centroid of these embeddings\n",
    "    centroid = calculate_centroid(label_embeddings)\n",
    "\n",
    "    # Calculate the distance of each image's embedding to the centroid\n",
    "    distances = [np.linalg.norm(e - centroid) for e in label_embeddings]\n",
    "\n",
    "    # Find the index of the minimum distance\n",
    "    min_index = np.argmin(distances)\n",
    "\n",
    "    # Return the image corresponding to this index\n",
    "    central_image = [d['image'] for d in face_data_manager.data if d['label'] == str(label)][min_index]\n",
    "\n",
    "    return central_image\n",
    "\n",
    "# Load video\n",
    "cap = cv2.VideoCapture(video_output)\n",
    "total_frames = int(len(processed_images))\n",
    "\n",
    "# Define an output widget for the frame display\n",
    "frame_output = widgets.Output()\n",
    "\n",
    "# At the start of your code, add the following line to get the first frame that has a label '0'\n",
    "first_frame_with_label_0 = min([f['frame'] for f in face_data_manager.data if f['label'] == '0' or f['label'] == ''], default=0)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "# Define a slider to navigate frames\n",
    "slider = widgets.IntSlider(min=0, max=int(total_frames-1), step=fps, description='Frame:')\n",
    "\n",
    "def on_slider_change(change):\n",
    "    process_frame(change['new'])\n",
    "\n",
    "slider.observe(on_slider_change, names='value')\n",
    "\n",
    "# Display the slider and frame_output widget\n",
    "display(slider, frame_output)\n",
    "slider.value = first_frame_with_label_0\n",
    "# Process the first frame with label '0'\n",
    "\"\"\" process_frame(first_frame_with_label_0) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "\"\"\" data = [] \"\"\"\n",
    "\"\"\" for label in os.listdir(character_folder):\n",
    "    for filename in os.listdir(os.path.join(character_folder, label)):\n",
    "        if filename.endswith('.npy'):  # check if the file is an image\n",
    "            # read the image\n",
    "            img_path = os.path.join(character_folder, label, filename)\n",
    "            embedding = np.load(img_path)\n",
    "            data.append({'embedding': embedding, 'label': label})\n",
    "                \n",
    " \"\"\"\n",
    "# create a dataframe\n",
    "df = pd.DataFrame(face_data_manager.data)\n",
    "\n",
    "# Separate features and target\n",
    "X = np.array(df['normed_embedding'].to_list()) # Convert list of embeddings back to numpy array\n",
    "y = df['label'].values\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "# Normalize the embeddings\n",
    "normalizer = Normalizer(norm='l2')\n",
    "X = normalizer.transform(X)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2)\n",
    "\n",
    "# Define the model\n",
    "knn = KNeighborsClassifier(n_neighbors=5,metric=\"cosine\",weights=\"distance\")\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model\n",
    "accuracy = knn.score(X_val, y_val)\n",
    "print(f'Validation accuracy: {accuracy}')\n",
    "def classify_face(embedding):\n",
    "    normalizer = Normalizer(norm='l2')\n",
    "    embedding = normalizer.transform([embedding])\n",
    "    predicted_label = knn.predict(embedding)\n",
    "    predicted_label_str = le.inverse_transform(predicted_label)[0]\n",
    "\n",
    "    # Additional similarity check\n",
    "    similarity_threshold = 0.34\n",
    "    predicted_class_embeddings = X_train[y_train == predicted_label[0]]\n",
    "    sims_to_predicted_class = np.dot(predicted_class_embeddings, embedding[0])\n",
    "    if np.mean(sims_to_predicted_class) < similarity_threshold:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    return predicted_label_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular.all import *\n",
    "import os\n",
    "import sys\n",
    "\n",
    "class SuppressPrints:\n",
    "    def __enter__(self):\n",
    "        self.original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self.original_stdout\n",
    "# Create a dataframe from your data\n",
    "df = pd.DataFrame(face_data_manager.data)\n",
    "\n",
    "# Separate features and target\n",
    "X = np.array(df['normed_embedding'].to_list())  # Convert list of embeddings back to numpy array\n",
    "y = df['label'].values\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Create a DataFrame for use with Fastai\n",
    "df_fastai = pd.DataFrame(X)\n",
    "df_fastai['label'] = y_encoded\n",
    "\n",
    "# Define categorical and continuous variables\n",
    "# We don't have any categorical variables in this case, so the list is empty\n",
    "cat_names = []\n",
    "# All the columns of embeddings are treated as continuous variables\n",
    "cont_names = list(df_fastai.columns[:-1])  # Exclude 'label' column\n",
    "\n",
    "# Define your splits for training and validation sets\n",
    "splits = TrainTestSplitter(test_size=0.2)(range_of(df_fastai))\n",
    "\n",
    "# Create a TabularDataLoaders\n",
    "dls = TabularDataLoaders.from_df(df_fastai, y_names=\"label\", y_block = CategoryBlock, \n",
    "                                 cat_names=cat_names, cont_names=cont_names, splits=splits, bs=64)\n",
    "\n",
    "# Define the model architecture\n",
    "# Here we use a simple feedforward neural network with two hidden layers of size 200 and 100.\n",
    "learn = tabular_learner(dls, layers=[200,100], metrics=accuracy)\n",
    "\n",
    "# Train the model\n",
    "learn.fit_one_cycle(5)\n",
    "\n",
    "def classify_face(embedding):\n",
    "    # Convert the embedding to DataFrame\n",
    "    df_new = pd.DataFrame([embedding], columns=cont_names)\n",
    "    # Use the trained model to make the prediction\n",
    "    with SuppressPrints():\n",
    "        pred_class, pred_idx, _ = learn.predict(df_new.iloc[0])\n",
    "    # Convert predicted index to integer\n",
    "    predicted_label = int(pred_idx)\n",
    "    predicted_label_str = le.inverse_transform([predicted_label])[0]\n",
    "\n",
    "    # Additional similarity check\n",
    "    similarity_threshold = 0.34\n",
    "    predicted_class_embeddings = X[y == int(predicted_label)]\n",
    "    sims_to_predicted_class = np.dot(predicted_class_embeddings, embedding)\n",
    "    if np.mean(sims_to_predicted_class) < similarity_threshold:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    return predicted_label_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(frame_folder):\n",
    "    os.makedirs(frame_folder)\n",
    "# Extract frames from video\n",
    "vidcap = cv2.VideoCapture(video_output)\n",
    "success, image = vidcap.read()\n",
    "count = 0\n",
    "target_imgs = []  # a list of file paths to images of the target faces\n",
    "while success:\n",
    "    path = os.path.join(frame_folder, f\"{count}.jpg\")\n",
    "    target_imgs.append(path)\n",
    "    cv2.imwrite(path, image)  # save frame as JPEG file\n",
    "    success, image = vidcap.read()\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete if swapped folder is not empty\n",
    "if  os.path.exists(swapped_folder):\n",
    "    shutil.rmtree(swapped_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the frame folder if it doesn't exist\n",
    "if not os.path.exists(swapped_folder):\n",
    "    os.makedirs(swapped_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import natsort\n",
    "import cv2\n",
    "from collections import Counter, deque\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import build_montages\n",
    "\n",
    "swappings ={\n",
    "   'm':get_first_face('/notebooks/1629085462605.jpeg'),\n",
    "   'b':get_first_face('/notebooks/bcp.png'),\n",
    "   'd':get_first_face('/notebooks/DSCAAZZEA.png'),\n",
    "   'yd':get_first_face('/notebooks/2023-07-07 13.36.17.jpg'),\n",
    "   'ld': get_first_face('/notebooks/DSC09400.jpg'),\n",
    "   'l2':get_first_face('/notebooks/2023-07-03 22.46.08.jpg'),\n",
    "   'ylo':get_first_face('/notebooks/2023-07-07 13.36.21.jpg'),\n",
    "   'la':get_first_face('/notebooks/2023-07-03 22.47.10.jpg'),\n",
    "   'jo':get_first_face('/notebooks/jocp.png'),\n",
    "   'g':get_first_face('/notebooks/2023-07-08 14.11.53.jpg'),\n",
    "   'je':get_first_face('/notebooks/2023-07-03 22.47.24.jpg'),\n",
    "   'cat': get_first_face('/notebooks/http___prod.static9.net.au___media_2018_07_09_09_54_catto.jpg'),\n",
    "}\n",
    "targets={\n",
    " '0': 'd',\n",
    " '': None,\n",
    " 'Unknown': None,\n",
    " 'Unknow': None,\n",
    "}\n",
    "\n",
    "source_face = get_first_face('/notebooks/DSC06729.JPG')\n",
    "\n",
    "attribution = {}\n",
    "unused_keys = list(swappings.keys())\n",
    "for key in list(targets.keys()):\n",
    "    value = targets[key]\n",
    "    if value is not None:\n",
    "        unused_keys.remove(value)\n",
    "\n",
    "video = cv2.VideoCapture(video_output)\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "smoothing_window_size = 5\n",
    "face_classification_history = collections.defaultdict(lambda: deque(maxlen=int(fps)+1))\n",
    "\n",
    "start_frame = None\n",
    "end_frame = None\n",
    "frame_count = -1\n",
    "DEBUG = False\n",
    "results = []  # List to hold face images for montage\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break  \n",
    "    frame_count += 1\n",
    "    if start_frame is not None and frame_count < start_frame:\n",
    "        continue\n",
    "    if end_frame is not None and frame_count > end_frame:\n",
    "        break\n",
    "\n",
    "    faces = app.get(frame)\n",
    "    \"\"\"     faces = sorted(faces, key=lambda x: x.bbox[0]) \"\"\"\n",
    "    res = frame.copy()\n",
    "\n",
    "    used_indices = set()  # Keep track of used indices in this frame\n",
    "    face_info = []  # List to store face info (index and position)\n",
    "    for face in faces:\n",
    "        index = classify_face(face.normed_embedding)\n",
    "        if index in used_indices:\n",
    "            continue\n",
    "        used_indices.add(index)  # Mark this index as used\n",
    "        \"\"\"         face_classification_history[index].append(index)\n",
    "                counter = Counter(face_classification_history[index])\n",
    "                most_common_index = counter.most_common(1)[0][0] \"\"\"\n",
    "        most_common_index = index\n",
    "        found = False\n",
    "        bbox = face.bbox.astype(int)\n",
    "        face_info.append((index,bbox))\n",
    "        # Store index and position for later use\n",
    "        \"\"\"         cropped_face = frame[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "                if cropped_face.size < 0:\n",
    "                    continue \"\"\"\n",
    "        \"\"\"         if face.det_score < 0.5:\n",
    "                    continue \"\"\"\n",
    "        if most_common_index in targets:\n",
    "            found = True\n",
    "            value = targets[most_common_index]\n",
    "            if value is not None:\n",
    "                res = swapper.get(res, face, swappings[value], paste_back=True)\n",
    "        if not found:\n",
    "            if most_common_index in attribution:\n",
    "                found = True\n",
    "                value = swappings[attribution[most_common_index]]\n",
    "                if value is not None:\n",
    "                    res = swapper.get(res, face, swappings[attribution[most_common_index]], paste_back=True)\n",
    "            else:\n",
    "                print(\"index not found :\" ,most_common_index)\n",
    "        if not found and index != \"Unknown\":\n",
    "            if len(unused_keys) == 0:  \n",
    "                unused_keys = list(swappings.keys())\n",
    "                for key in list(targets.keys()):\n",
    "                    if key in unused_keys:\n",
    "                        unused_keys.remove(key)\n",
    "            random_key = random.choice(unused_keys)\n",
    "            unused_keys.remove(random_key)\n",
    "            print(\"index \",most_common_index,\" attribute to \",random_key, \" frame \", frame_count)\n",
    "            attribution[most_common_index] = random_key\n",
    "            random_value = swappings[random_key]\n",
    "            res = swapper.get(res, face, random_value, paste_back=True)\n",
    "    cv2.imwrite(osp.join(swapped_folder, '{}.jpg'.format(frame_count)), res)\n",
    "    if DEBUG:\n",
    "        debug = res.copy()\n",
    "        for info in face_info:\n",
    "            index, bbox = info\n",
    "            new_bbox = bbox \n",
    "            cv2.putText(debug, str(index), (int(new_bbox[0]), int(new_bbox[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "        debug =  cv2.resize(debug,  (400, 400), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "        cv2.putText(debug,str(frame_count), (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(debug,str(len(face_info)), (350, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "        results.append(debug)\n",
    "        # Every fps number of frames, create a montage and display it\n",
    "        if frame_count % int(fps) == 0:\n",
    "            montage = build_montages(results, (150, 150), (5, 5))[0]\n",
    "            # Calculate time elapsed in the video\n",
    "            time_elapsed = frame_count / fps  # time in seconds\n",
    "            minutes = int(time_elapsed // 60)\n",
    "            seconds = int(time_elapsed % 60)\n",
    "            print(f\"Current time in video: {minutes:02}:{seconds:02}\")\n",
    "            # Clear previous output and display the image\n",
    "            \"\"\"  clear_output(wait=True) \"\"\"\n",
    "            plt.figure(figsize=(400,400))\n",
    "            plt.imshow(cv2.cvtColor(montage, cv2.COLOR_BGR2RGB))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            results = []  # Clear the faces list\n",
    "    \n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_images = glob.glob(os.path.join(swapped_folder, '0*.jpg'))\n",
    "for file in processed_images:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "output_video = \"/notebooks/output.mp4\"\n",
    "video = cv2.VideoCapture(video_output)\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "video.release()\n",
    "\n",
    "# Get the processed images\n",
    "processed_images = glob.glob(os.path.join(swapped_folder, '*.jpg'))\n",
    "\n",
    "# Sort the processed images (this may be necessary depending on how your files are named)\n",
    "processed_images.sort()\n",
    "print(len(processed_images))\n",
    "# Initialize the video writer\n",
    "height, width, _ = cv2.imread(processed_images[0]).shape\n",
    "print(height, width)\n",
    "\n",
    "# Define the command\n",
    "command = f'ffmpeg -y -r {fps} -s {width}x{height} -i {swapped_folder}/%01d.jpg -vcodec libx264  -pix_fmt yuv420p {output_video}'\n",
    "\n",
    "# Execute the command\n",
    "subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "temp_audio_file = '/notebooks/temp_audio.aac'\n",
    "\n",
    "# Remove temporary audio file if it exists\n",
    "if os.path.exists(temp_audio_file):\n",
    "    os.remove(temp_audio_file)\n",
    "\n",
    "# Remove output video with audio file if it exists\n",
    "output_with_audio_file = '/notebooks/output_with_audio.mp4'\n",
    "if os.path.exists(output_with_audio_file):\n",
    "    os.remove(output_with_audio_file)\n",
    "\n",
    "# Extract audio from original video and save it as a temporary audio file\n",
    "audio_extraction_command = f'ffmpeg -y -i {video_output} -vn -acodec aac -strict -2 {temp_audio_file}'\n",
    "subprocess.run(audio_extraction_command, shell=True)\n",
    "\n",
    "# Combine swapped video with original audio\n",
    "video_combination_command = f'ffmpeg -y -i {output_video} -i {temp_audio_file} -c:v copy -c:a copy -map 0:v:0 -map 1:a:0 {output_with_audio_file}'\n",
    "subprocess.run(video_combination_command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "\n",
    "# path to the video file\n",
    "video_path = output_with_audio_file\n",
    "\n",
    "# specify the start and end times in seconds\n",
    "# start time as (minutes, seconds)\n",
    "start_time_min_sec = (0, 0)  # 2 minutes 30 seconds\n",
    "start_time = start_time_min_sec[0]*60 + start_time_min_sec[1]\n",
    "\n",
    "# end time as (minutes, seconds)\n",
    "end_time_min_sec = (1, 40)  # 3 minutes 45 seconds\n",
    "end_time = end_time_min_sec[0]*60 + end_time_min_sec[1]\n",
    "\n",
    "# output file path\n",
    "output_path = \"/notebooks/split.mp4\"\n",
    "\n",
    "# extract subclip\n",
    "ffmpeg_extract_subclip(video_path, start_time, end_time, targetname=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONVERT GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imageio\n",
    "\n",
    "# Read the gif file\n",
    "gif = imageio.mimread('input.gif')\n",
    "\n",
    "# Convert the gif to .mp4\n",
    "with imageio.get_writer('output.mp4', mode='I') as writer:\n",
    "    for frame in gif:\n",
    "        writer.append_data(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
