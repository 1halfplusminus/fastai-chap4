{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U insightface onnxruntime-gpu  numpy fastai pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U gdown moviepy ffmpeg insightface natsort hdbscan imutils  seaborn annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!sudo apt update\n",
    "!sudo apt install streamlink -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import cv2\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip,AudioFileClip\n",
    "import glob\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "from insightface.data import get_image as ins_get_image\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from imutils import build_montages\n",
    "from sklearn.cluster import DBSCAN\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download video from Google Drive\n",
    "url = f'https://drive.google.com/uc?id=18wEUfMNohBJ4K3Ly5wpTejPfDzp-8fI8'\n",
    "gdown.download(url, \"antilop.zip\", quiet=False)\n",
    "!unzip antilop.zip -d antilop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define variables\n",
    "drive_file_id = '1kgIl4rCSaVQ_PLrMfa4AJQPo5YuKFwFO'  # replace 'FILE_ID' with your file's ID\n",
    "video_output = 'my_video.mp4'\n",
    "frame_folder = '/notebooks/frame_folder'\n",
    "swapped_folder = '/notebooks/swapped'\n",
    "character_folder = '/notebooks/character'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(frame_folder)\n",
    "shutil.rmtree(swapped_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert insightface.__version__ >= '0.7'\n",
    "\n",
    "\n",
    "# Create the frame folder if it doesn't exist\n",
    "if not os.path.exists(frame_folder):\n",
    "    os.makedirs(frame_folder)\n",
    "# Create the frame folder if it doesn't exist\n",
    "if not os.path.exists(swapped_folder):\n",
    "    os.makedirs(swapped_folder)\n",
    "# Your face swapping script\n",
    "app = FaceAnalysis(model='./antilop')\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "swapper = insightface.model_zoo.get_model('/notebooks/inswapper_128.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_first_face(image):\n",
    "    # If the image is a string (presumably a file path), read the image\n",
    "    if isinstance(image, str):\n",
    "        image = cv2.imread(image)\n",
    "\n",
    "    # Check if the image is a valid numpy array\n",
    "    if isinstance(image, np.ndarray):\n",
    "        source_faces = app.get(image)\n",
    "        source_faces = sorted(source_faces, key=lambda x: x.bbox[0])\n",
    "        if len(source_faces) == 0:\n",
    "            print(image)\n",
    "            assert False\n",
    "\n",
    "        return source_faces[0]\n",
    "    print(image)\n",
    "    assert False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://example.com/path_to_your_file\"  # replace with your file's URL\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(video_output, 'wb') as f:  # replace with the path where you want to save the file\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "# Download video from Google Drive\n",
    "url = f'https://drive.google.com/uc?id={drive_file_id}'\n",
    "gdown.download(url, video_output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install  streamlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = f'''-o {video_output} \"https://www.youtube.com/watch?v=_q5vw2MupE0\" 360p'''\n",
    "!python -m streamlink {cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete if swapped folder is not empty\n",
    "if  os.path.exists(character_folder):\n",
    "    shutil.rmtree(character_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "import datetime\n",
    "# Create the frame folder if it doesn't exist\n",
    "if not os.path.exists(character_folder):\n",
    "    os.makedirs(character_folder)\n",
    "if not os.path.exists(frame_folder):\n",
    "    os.makedirs(frame_folder)\n",
    "# Initialize the FaceAnalysis application\n",
    "\"\"\" app = FaceAnalysis(providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))  # Use GPU device 0 and input image size as (640, 640) \"\"\"\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_output)\n",
    "# Calculate frame rate (frames per second)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "face_index= 0\n",
    "# Initialize a list to store embeddings and face data for each detected face\n",
    "face_data = []\n",
    "count = 0\n",
    "# Loop through the video file frame by frame\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        # Calculate the time in minutes and seconds\n",
    "    time_in_milliseconds = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "    frame_time = datetime.timedelta(milliseconds=time_in_milliseconds)\n",
    "    path = os.path.join(frame_folder, f\"{count}.jpg\")\n",
    "    cv2.imwrite(path, frame)  # save frame as JPEG file\n",
    "    # Use the FaceAnalysis application to detect faces in the frame\n",
    "    faces = app.get(frame)\n",
    "\n",
    "    # For each detected face, extract the embedding, bounding box, and face image, and add them to the list\n",
    "    for face in faces:\n",
    "        face_embedding = face.embedding\n",
    "        bbox = face.bbox.astype(int)\n",
    "        cropped_face = frame[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "        if cropped_face.size > 0:\n",
    "            face_data.append({'index':face_index,'label':'','frame':count,'time':frame_time,'face_embedding': face_embedding,'score':face.det_score,'normed_embedding': face.normed_embedding, 'bbox': bbox, 'image': cv2.cvtColor(cropped_face, cv2.COLOR_BGR2RGB)})\n",
    "        face_index += 1\n",
    "    \n",
    "    count += 1\n",
    "\n",
    "# Save the face_data to disk\n",
    "with open('face_data.pkl', 'wb') as f:\n",
    "    pickle.dump(face_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "class FaceDataManager:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        with open(self.file_path, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "\n",
    "    def save_data(self):\n",
    "        with open(self.file_path, 'wb') as f:\n",
    "            pickle.dump(self.data, f)\n",
    "\n",
    "    def update_label(self, frame_number, face_image, new_label,save= True):\n",
    "        for f in self.data:\n",
    "            if f['frame'] == frame_number and np.array_equal(f['image'], face_image):\n",
    "                f['label'] = new_label\n",
    "        if save:\n",
    "            self.save_data()\n",
    "    def update_labels(self,old_label ,new_label):\n",
    "        for f in self.data:\n",
    "            if f['label'] == old_label:\n",
    "                f['label'] = new_label\n",
    "        self.save_data()\n",
    "    \n",
    "global face_data_manager\n",
    "face_data_manager = FaceDataManager('face_data.pkl')\n",
    "labels = [f['label'] for f in face_data_manager.data]\n",
    "# get unique labels\n",
    "\"\"\" labels = list(set(labels)) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "face_data_manager.load_data()\n",
    "\n",
    "data = []\n",
    "images = []\n",
    "video = []\n",
    "chunk = []\n",
    "prev_time = None\n",
    "\n",
    "for face in face_data_manager.data:\n",
    "    current_time = face['time']\n",
    "\n",
    "    # If it's the first face or at least one second has passed, process the face\n",
    "    # if prev_time is None or (current_time - prev_time).total_seconds() >= 0.5:\n",
    "    data.append(face['normed_embedding'])\n",
    "    images.append(face['image'])\n",
    "    video.append(\"output.mp4\")\n",
    "    chunk.append(1)\n",
    "    prev_time = current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "def scatter_thumbnails(data, images, zoom=0.12, colors=None):\n",
    "    assert len(data) == len(images)\n",
    "\n",
    "    # reduce embedding dimensions to 2\n",
    "    x = PCA(n_components=2).fit_transform(data) if len(data[0]) > 2 else data\n",
    "\n",
    "    # create a scatter plot.\n",
    "    f = plt.figure(figsize=(22, 15))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], s=4)\n",
    "    _ = ax.axis('off')\n",
    "    _ = ax.axis('tight')\n",
    "\n",
    "    # add thumbnails :)\n",
    "    for i in range(len(images)):\n",
    "        image = images[i]\n",
    "        im = OffsetImage(image, zoom=zoom)\n",
    "        bboxprops = dict(edgecolor=colors[i]) if colors is not None else None\n",
    "        ab = AnnotationBbox(im, x[i], xycoords='data',\n",
    "                            frameon=(bboxprops is not None),\n",
    "                            pad=0.02,\n",
    "                            bboxprops=bboxprops)\n",
    "        ax.add_artist(ab)\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Call the scatter_thumbnails function\n",
    "scatter_thumbnails(data, images)\n",
    "plt.title('Facial Embeddings - Principal Component Analysis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# PCA first to speed it up\n",
    "x = PCA(n_components=4).fit_transform(data)\n",
    "x = TSNE(perplexity=9,\n",
    "         n_components=3).fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = scatter_thumbnails(x, images, zoom=0.25)\n",
    "plt.title('3D t-Distributed Stochastic Neighbor Embedding')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q hdbscan\n",
    "import hdbscan\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import OPTICS\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def calculate_cosine_distance(feat1, feat2):\n",
    "    feat1 = feat1.ravel()\n",
    "    feat2 = feat2.ravel()\n",
    "    sim = np.dot(feat1, feat2) / (norm(feat1) * norm(feat2))\n",
    "    return 1 - sim\n",
    "def plot_clusters(data, algorithm, *args, **kwds):\n",
    "    cluster = algorithm(*args, **kwds)\n",
    "    labels = algorithm(*args, **kwds).fit_predict(data)\n",
    "    palette = sns.color_palette('deep', np.max(labels) + 1)\n",
    "    colors = [palette[x] if x >= 0 else (0,0,0) for x in labels]\n",
    "    # ax = scatter_thumbnails(x, images, 0.06, colors)\n",
    "    plt.title(f'Clusters found by {algorithm.__name__}')\n",
    "    return cluster ,labels\n",
    "\n",
    "\"\"\" clustering,labels = plot_clusters(x, hdbscan.HDBSCAN, alpha=1.0, min_cluster_size=10, min_samples=70)  \"\"\"\n",
    "\"\"\" clusters = plot_clusters(x, cluster.DBSCAN, n_jobs=-1, eps=0.52, min_samples=1) \"\"\"\n",
    "\"\"\" clusters = plot_clusters(x, AffinityPropagation) \"\"\"\n",
    "\"\"\" clustering = plot_clusters(x, OPTICS, min_samples=120)  \"\"\"\n",
    "\"\"\" clusters = plot_clusters(x, MeanShift) \"\"\"\n",
    "\n",
    "clustering,labels = plot_clusters(x, hdbscan.HDBSCAN, alpha=0.82, min_cluster_size=10, min_samples=120,metric=calculate_cosine_distance)  \n",
    "for i,label in enumerate(labels):\n",
    "    face_data_manager.data[i][\"label\"]= str(label.item())\n",
    "face_data_manager.save_data()\n",
    "list(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "\n",
    "df= pd.DataFrame(face_data_manager.data)\n",
    "X= x\n",
    "f = len(X[0])\n",
    "t = AnnoyIndex(f, metric=\"dot\")\n",
    "ntree = 8 \n",
    "\n",
    "for i, vector in enumerate(X):\n",
    "    t.add_item(i, vector)\n",
    "_  = t.build(ntree)\n",
    "def get_similar_images_annoy(img_index, n=8, max_dist=0.83):\n",
    "    vid, face  = df.iloc[img_index, [0,8]]\n",
    "    similar_img_ids, dist = t.get_nns_by_item(img_index, n+1, include_distances=True)\n",
    "    similar_img_ids = [s for s,d in zip(similar_img_ids, dist) if d <= max_dist][1:]  # first item is always its own video\n",
    "    return vid, face, df.iloc[similar_img_ids], dist\n",
    "def get_sample_n_similar(sample_idx):\n",
    "    vid, face, similar, distances = get_similar_images_annoy(sample_idx)\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    gs = fig.add_gridspec(2, 6)\n",
    "    ax1 = fig.add_subplot(gs[0:2, 0:2])\n",
    "    ax2 = fig.add_subplot(gs[0, 2])\n",
    "    ax3 = fig.add_subplot(gs[0, 3])\n",
    "    ax4 = fig.add_subplot(gs[0, 4])\n",
    "    ax5 = fig.add_subplot(gs[0, 5])\n",
    "    ax6 = fig.add_subplot(gs[1, 2])\n",
    "    ax7 = fig.add_subplot(gs[1, 3])\n",
    "    ax8 = fig.add_subplot(gs[1, 4])\n",
    "    ax9 = fig.add_subplot(gs[1, 5])\n",
    "    axx = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9]\n",
    "    for ax in axx:\n",
    "        ax.set_axis_off()\n",
    "    list_plot = [face] + similar['image'].values.tolist()\n",
    "    list_cluster = [df.iloc[sample_idx]['label']] + similar['label'].values.tolist()\n",
    "    for ax, face, cluster, dist in zip(axx, list_plot, list_cluster, distances):\n",
    "        ax.imshow(face)\n",
    "        ax.set_title(f' @{dist:.2f}\\label:{cluster}') # show video filename and distance\n",
    "# display samples and their nearest neighbors\n",
    "for i in np.random.choice(len(X), 10, replace=False):\n",
    "    get_sample_n_similar(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold mappings of old cluster to new cluster\n",
    "merge_map = {}\n",
    "df = pd.DataFrame(face_data_manager.data)\n",
    "# Iterate over all clusters\n",
    "for cluster in df['label'].unique():\n",
    "    # Get representative item for this cluster (for simplicity, we choose the first one)\n",
    "    representative_item = df[df['label'] == cluster].index[0]\n",
    "\n",
    "    # Get the closest clusters using Annoy\n",
    "    similar_items, distances = t.get_nns_by_item(representative_item, n=9, include_distances=True)\n",
    "\n",
    "    # Check the second closest item's distance (the closest will be the item itself)\n",
    "    for dist in distances:\n",
    "        if dist <= 0.28:\n",
    "            similar_item = similar_items[1]\n",
    "            similar_cluster = df.loc[similar_item, 'label']\n",
    "            merge_map[cluster] = similar_cluster\n",
    "    else:\n",
    "        # Otherwise, this cluster stays independent\n",
    "        merge_map[cluster] = cluster\n",
    "\n",
    "# Now update the 'cluster' field using the merge_map\n",
    "df = pd.DataFrame(face_data_manager.data)\n",
    "print(df[\"label\"].nunique())\n",
    "for i,data in enumerate(face_data_manager.data):\n",
    "    face_data_manager.data[i][\"label\"]= merge_map[data[\"label\"]]\n",
    "face_data_manager.save_data()\n",
    "df = pd.DataFrame(face_data_manager.data)\n",
    "\n",
    "df[\"label\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Delete if swapped folder is not empty\n",
    "if  os.path.exists(character_folder):\n",
    "    shutil.rmtree(character_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for label in list(set(labels)):\n",
    "    # Skip the noise\n",
    "    if label == -1:\n",
    "        continue  \n",
    "\n",
    "    directory_path = os.path.join(character_folder, str(label))\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "    # Select samples associated with the current label\n",
    "    idxs = np.where(labels == label)[0]\n",
    "    faces = []\n",
    "    # loop over the sampled indexes\n",
    "    for i in idxs:\n",
    "        face_data_manager.data[i]['label'] = str(label)\n",
    "    idxs = np.random.choice(idxs, size=min(25, len(idxs)), replace=False)\n",
    "    for i in idxs:\n",
    "        # Get the time elapsed\n",
    "        time_elapsed = face_data_manager.data[i]['time']\n",
    "        minutes = time_elapsed.total_seconds() // 60\n",
    "        seconds = time_elapsed.total_seconds() % 60\n",
    "        # Calculate minutes and seconds\n",
    "        minutes = time_elapsed.total_seconds() // 60\n",
    "        seconds = time_elapsed.total_seconds() % 60\n",
    "        # Get the face from the data\n",
    "        face = face_data_manager.data[i][\"image\"]\n",
    "        # Format the time string as MM:SS\n",
    "        frame_time = \"{:02}:{:02}\".format(int(minutes), int(seconds))\n",
    "        # Force resize the face to 96x96 and then add it to the\n",
    "        # faces montage list\n",
    "        face = cv2.resize(face, (150, 150))\n",
    "        cv2.putText(face,frame_time, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "        faces.append(face)\n",
    "    montage = build_montages(faces, (150, 150), (5, 5))[0]\n",
    "    # Save the output montage\n",
    "    title = \"Face ID #{}\".format(label)\n",
    "    title = \"Unknown Faces\" if label == -1 else title\n",
    "    cv2.imwrite(os.path.join(directory_path, title+'.jpg'), montage)\n",
    "    \n",
    "face_data_manager.save_data()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from fastai.vision.all import *\n",
    "from pathlib import Path\n",
    "from PIL import Image as PilImage\n",
    "import shutil\n",
    "\n",
    "path = Path(character_folder)\n",
    "labels = [data[\"label\"] for data in face_data_manager.data if data[\"label\"]]\n",
    "\n",
    "def rename_folder(old_name, new_name):\n",
    "    if new_name.exists():  # If the target directory exists\n",
    "     \n",
    "        shutil.rmtree(old_name)\n",
    "    else:\n",
    "        old_name.rename(new_name)\n",
    "        \n",
    "def create_on_button_clicked_handler(label, name):\n",
    "    global face_data_manager\n",
    "    def on_button_clicked(b):\n",
    "        global face_data_manager\n",
    "        new_label = name.value\n",
    "        print(f'Character name confirmed: {new_label}')\n",
    "\n",
    "        face_data_manager.update_labels(label, new_label)\n",
    "\n",
    "        old_name = Path(Path(path) / label)\n",
    "        new_name = Path(Path(path) / new_label)\n",
    "\n",
    "        rename_folder(old_name, new_name)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        next(process)\n",
    "\n",
    "    return on_button_clicked\n",
    "def process_folders(labels, path):\n",
    "    global face_data_manager\n",
    "    for label in labels:\n",
    "        image_paths = list((path / label).glob('*.jpg'))\n",
    "        if not image_paths:\n",
    "            continue\n",
    "        print(f\"Current cluster: {label}\")\n",
    "\n",
    "        image_resolution = {}\n",
    "        for image_path in image_paths:\n",
    "            with PilImage.open(image_path) as img:\n",
    "                width, height = img.size\n",
    "                resolution = width * height  # resolution is defined as width * height\n",
    "                image_resolution[image_path] = resolution\n",
    "\n",
    "        sorted_image_paths = sorted(image_resolution, key=image_resolution.get, reverse=True)\n",
    "        name = widgets.Text(value=label, placeholder='Enter character name', description='Name:')\n",
    "        button = widgets.Button(description='Confirm')\n",
    "        display(name, button)\n",
    "        with open(sorted_image_paths[0], \"rb\") as file:\n",
    "            image = file.read()\n",
    "            img_widget = widgets.Image(value=image, format='png', width=1000, height=400)\n",
    "            display(img_widget)\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "        button.on_click(create_on_button_clicked_handler(label, name))\n",
    "  \n",
    "        yield\n",
    "\n",
    "process = process_folders(labels, path)\n",
    "next(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in face_data_manager.data:\n",
    "    if data[\"label\"].isnumeric():\n",
    "        print(data[\"index\"],\"is numeric\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "\"\"\" data = [] \"\"\"\n",
    "\"\"\" for label in os.listdir(character_folder):\n",
    "    for filename in os.listdir(os.path.join(character_folder, label)):\n",
    "        if filename.endswith('.npy'):  # check if the file is an image\n",
    "            # read the image\n",
    "            img_path = os.path.join(character_folder, label, filename)\n",
    "            embedding = np.load(img_path)\n",
    "            data.append({'embedding': embedding, 'label': label})\n",
    "                \n",
    " \"\"\"\n",
    "# create a dataframe\n",
    "df = pd.DataFrame(face_data_manager.data)\n",
    "\n",
    "# Separate features and target\n",
    "X = x # Convert list of embeddings back to numpy array\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Encode labels\n",
    "labelencoder = LabelEncoder()\n",
    "y_encoded = labelencoder.fit_transform(y)\n",
    "# Normalize the embeddings\n",
    "normalizer = Normalizer(norm='l2')\n",
    "X = normalizer.transform(X)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2)\n",
    "\n",
    "# Define the model\n",
    "knn = KNeighborsClassifier(n_neighbors=9,metric=\"euclidean\")\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model\n",
    "accuracy = knn.score(X_val, y_val)\n",
    "print(f'Validation accuracy: {accuracy}')\n",
    "def retrain_knn():\n",
    "    global knn\n",
    "    global y\n",
    "    X = x # Convert list of embeddings back to numpy array\n",
    "    y = df[\"label\"]\n",
    "\n",
    "    # Encode labels\n",
    "    labelencoder = LabelEncoder()\n",
    "    y_encoded = labelencoder.fit_transform(y)\n",
    "    # Normalize the embeddings\n",
    "    normalizer = Normalizer(norm='l2')\n",
    "    X = normalizer.transform(X)\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2)\n",
    "\n",
    "    # Define the model\n",
    "    knn = KNeighborsClassifier(n_neighbors=9,metric=\"euclidean\")\n",
    "\n",
    "    # Train the model\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "# def classify_face(embedding,face_index = None):\n",
    "#     \"\"\"     normalizer = Normalizer(norm='l2')\n",
    "#     embedding = normalizer.transform([embedding]) \"\"\"\n",
    "#     embedding = [embedding]\n",
    "#     predicted_label = knn.predict(embedding)\n",
    "#     predicted_label_str = labelencoder.inverse_transform(predicted_label)[0]\n",
    "#     if predicted_label_str == \"-1\":\n",
    "#         annoy_similar_indices, dist = t.get_nns_by_vector(embedding[0], 9, include_distances=True)\n",
    "#         # Check if the similarity based on Annoy index is above the threshold\n",
    "#         max_dist = 1\n",
    "#         if dist[1] <= max_dist:\n",
    "#             return labels[annoy_similar_indices[1]]\n",
    "#         return \"Unknown\"\n",
    "#     # Additional similarity check\n",
    "#     \"\"\" imilarity_threshold = 0.34\n",
    "#         predicted_class_embeddings = X_train[y_train == predicted_label[0]]\n",
    "#         sims_to_predicted_class = np.dot(predicted_class_embeddings, embedding[0])\n",
    "#         if np.mean(sims_to_predicted_class) < similarity_threshold:\n",
    "#             annoy_similar_indices, dist = t.get_nns_by_vector(embedding[0], 2, include_distances=True)\n",
    "#             # Check if the similarity based on Annoy index is above the threshold\n",
    "#             max_dist = 1\n",
    "#             if dist[1] <= max_dist:\n",
    "#                 embedding = x[annoy_similar_indices[1]]\n",
    "#                 return classify_face(embedding[0],face_index)\n",
    "#             return \"Unknown\"\n",
    "#     \"\"\"\n",
    "#     return predicted_label_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_face(embedding,face_index = None):\n",
    "    global face_data_manager\n",
    "    # normalizer = Normalizer(norm='l2')\n",
    "    # embedding = normalizer.transform([embedding])\n",
    "    annoy_similar_indices, dist = t.get_nns_by_vector(embedding, 10, include_distances=True)\n",
    "    predicted_label_str = face_data_manager.data[annoy_similar_indices[0]][\"label\"]\n",
    "    if predicted_label_str == \"-1\":\n",
    "        for i in range(1,len(annoy_similar_indices)):\n",
    "            if face_data_manager.data[annoy_similar_indices[i]][\"label\"] != \"-1\":\n",
    "                return face_data_manager.data[annoy_similar_indices[i]][\"label\"] \n",
    "\n",
    "    # Additional similarity check\n",
    "    similarity_threshold = 0.28\n",
    "    predicted_class_embeddings = X_train[y_train == predicted_label_str]\n",
    "    sims_to_predicted_class = np.dot(predicted_class_embeddings, embedding[0])\n",
    "    if np.mean(sims_to_predicted_class) < similarity_threshold:\n",
    "        return \"-1\"\n",
    "\n",
    "    return predicted_label_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(frame_folder):\n",
    "    os.makedirs(frame_folder)\n",
    "# Extract frames from video\n",
    "vidcap = cv2.VideoCapture(video_output)\n",
    "success, image = vidcap.read()\n",
    "count = 0\n",
    "target_imgs = []  # a list of file paths to images of the target faces\n",
    "while success:\n",
    "    path = os.path.join(frame_folder, f\"{count}.jpg\")\n",
    "    target_imgs.append(path)\n",
    "    cv2.imwrite(path, image)  # save frame as JPEG file\n",
    "    success, image = vidcap.read()\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete if swapped folder is not empty\n",
    "if  os.path.exists(swapped_folder):\n",
    "    shutil.rmtree(swapped_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the frame folder if it doesn't exist\n",
    "if not os.path.exists(swapped_folder):\n",
    "    os.makedirs(swapped_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import natsort\n",
    "import cv2\n",
    "from collections import Counter, deque\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import build_montages\n",
    "global swappings\n",
    "swappings ={\n",
    "   'm':get_first_face('/notebooks/swapping/m.jpg'),\n",
    "   'g':get_first_face('/notebooks/swapping/g.jpg'),\n",
    "   'la':get_first_face('/notebooks/swapping/la.jpg'),\n",
    "   'l':get_first_face('/notebooks/swapping/l.jpg'),\n",
    "   'je':get_first_face('/notebooks/swapping/je.jpg'),\n",
    "   'b':get_first_face('/notebooks/swapping/b.jpg'),\n",
    "   'j':get_first_face('/notebooks/swapping/j.jpg'),\n",
    "   'yb':get_first_face('/notebooks/swapping/yb.jpg'),\n",
    "   'yb':get_first_face('/notebooks/swapping/yb.jpg'),\n",
    "   'yl':get_first_face('/notebooks/swapping/yl.jpg'),\n",
    "}\n",
    "targets={\n",
    "\"-1\":None\n",
    "}\n",
    "video = cv2.VideoCapture(video_output)\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "smoothing_window_size = 5\n",
    "face_classification_history = collections.defaultdict(lambda: deque(maxlen=int(fps)+1))\n",
    "\n",
    "start_frame = None\n",
    "end_frame = None\n",
    "frame_count = -1\n",
    "DEBUG = False\n",
    "results = []  # List to hold face images for montage\n",
    "face_index = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" source_face = get_first_face('/notebooks/DSC06729.JPG') \"\"\"\n",
    "# Define your file path\n",
    "import json\n",
    "\n",
    "\n",
    "attribution_path = \"attribution.json\"\n",
    "# Check if the file exists\n",
    "if os.path.exists(attribution_path):\n",
    "    # Load the attribution data from the file\n",
    "    with open(attribution_path, 'r') as f:\n",
    "        attribution = json.load(f)\n",
    "else:\n",
    "    # Create a new empty attribution dictionary\n",
    "    attribution = {}\n",
    "unused_keys = list(swappings.keys())\n",
    "for key in list(targets.keys()):\n",
    "    value = targets[key]\n",
    "    if value is not None:\n",
    "        unused_keys.remove(value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break  \n",
    "    frame_count += 1\n",
    "    if start_frame is not None and frame_count < start_frame:\n",
    "        continue\n",
    "    if end_frame is not None and frame_count > end_frame:\n",
    "        break\n",
    "\n",
    "    faces = app.get(frame)\n",
    "    \"\"\"     faces = sorted(faces, key=lambda x: x.bbox[0]) \"\"\"\n",
    "    res = frame.copy()\n",
    "\n",
    "    used_indices = set()  # Keep track of used indices in this frame\n",
    "    face_info = []  # List to store face info (index and position)\n",
    "    for face in faces:\n",
    "        index = classify_face(face.normed_embedding,face_index=face_index)\n",
    "        # if index in used_indices:\n",
    "        #     continue\n",
    "        used_indices.add(index)  # Mark this index as used\n",
    "        \"\"\"         face_classification_history[index].append(index)\n",
    "                counter = Counter(face_classification_history[index])\n",
    "                most_common_index = counter.most_common(1)[0][0] \"\"\"\n",
    "        most_common_index = index\n",
    "        found = False\n",
    "        bbox = face.bbox.astype(int)\n",
    "        face_info.append((index,bbox))\n",
    "        # Store index and position for later use\n",
    "        \"\"\"         cropped_face = frame[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "                if cropped_face.size < 0:\n",
    "                    continue \"\"\"\n",
    "        \"\"\"         if face.det_score < 0.5:\n",
    "                    continue \"\"\"\n",
    "        if most_common_index in targets:\n",
    "            found = True\n",
    "            value = targets[most_common_index]\n",
    "            if value is not None:\n",
    "                res = swapper.get(res, face, swappings[value], paste_back=True)\n",
    "        if not found:\n",
    "            if most_common_index in attribution:\n",
    "                found = True\n",
    "                value = swappings[attribution[most_common_index]]\n",
    "                if value is not None:\n",
    "                    res = swapper.get(res, face, swappings[attribution[most_common_index]], paste_back=True)\n",
    "            else:\n",
    "                print(\"index not found :\" ,most_common_index)\n",
    "        if not found and index != \"Unknown\":\n",
    "            if len(unused_keys) == 0:  \n",
    "                unused_keys = list(swappings.keys())\n",
    "                for key in list(targets.keys()):\n",
    "                    if key in unused_keys:\n",
    "                        unused_keys.remove(key)\n",
    "            random_key = random.choice(unused_keys)\n",
    "            unused_keys.remove(random_key)\n",
    "            print(\"index \",most_common_index,\" attribute to \",random_key, \" frame \", frame_count)\n",
    "            attribution[most_common_index] = random_key\n",
    "            random_value = swappings[random_key]\n",
    "            res = swapper.get(res, face, random_value, paste_back=True)\n",
    "        face_index += 1\n",
    "    cv2.imwrite(osp.join(swapped_folder, '{}.jpg'.format(frame_count)), res)\n",
    "    if DEBUG:\n",
    "        debug = res.copy()\n",
    "        for info in face_info:\n",
    "            index, bbox = info\n",
    "            new_bbox = bbox \n",
    "            cv2.putText(debug, str(index), (int(new_bbox[0]), int(new_bbox[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "        debug =  cv2.resize(debug,  (400, 400), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "        cv2.putText(debug,str(frame_count), (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(debug,str(len(face_info)), (350, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "        results.append(debug)\n",
    "        # Every fps number of frames, create a montage and display it\n",
    "        if frame_count % int(fps) == 0:\n",
    "            montage = build_montages(results, (150, 150), (5, 5))[0]\n",
    "            # Calculate time elapsed in the video\n",
    "            time_elapsed = frame_count / fps  # time in seconds\n",
    "            minutes = int(time_elapsed // 60)\n",
    "            seconds = int(time_elapsed % 60)\n",
    "            print(f\"Current time in video: {minutes:02}:{seconds:02}\")\n",
    "            # Clear previous output and display the image\n",
    "            \"\"\"  clear_output(wait=True) \"\"\"\n",
    "            plt.figure(figsize=(400,400))\n",
    "            plt.imshow(cv2.cvtColor(montage, cv2.COLOR_BGR2RGB))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            results = []  # Clear the faces list\n",
    "\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated attribution data back to the file\n",
    "with open(attribution_path, 'w') as f:\n",
    "    json.dump(attribution, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from ipywidgets import HBox, VBox\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from fastai.vision.all import *\n",
    "from pathlib import Path\n",
    "from PIL import Image as PilImage\n",
    "import shutil\n",
    "from ipywidgets import GridBox\n",
    "\n",
    "\"\"\" attribution = {} \"\"\"\n",
    "\"\"\" unused_keys = list(swappings.keys()) \"\"\"\n",
    "\n",
    "frame_output = widgets.Output()\n",
    "path = Path(character_folder)\n",
    "processed_images = glob.glob(os.path.join(frame_folder, '*.jpg'))\n",
    "processed_images.sort(key=lambda f: int(re.sub('\\D', '', os.path.basename(f))))\n",
    "def calculate_cosine_similarity(feat1, feat2):\n",
    "    feat1 = feat1.ravel()\n",
    "    feat2 = feat2.ravel()\n",
    "    sim = np.dot(feat1, feat2) / (norm(feat1) * norm(feat2))\n",
    "    return sim\n",
    "# -------------------------------- Function definitions ------------------------------\n",
    "def get_class_label_embedding(label):\n",
    "    # Gather all embeddings with this label\n",
    "    embeddings = [d['normed_embedding'] for d in face_data_manager.data if d['label'] == str(label)]\n",
    "    # Calculate the average embedding\n",
    "    average_embedding = np.mean(embeddings, axis=0)\n",
    "    return average_embedding\n",
    "\n",
    "def create_on_button_clicked(face, frame_number, name, path, character_folder):\n",
    "    def on_button_clicked(b):\n",
    "        global face_data_manager\n",
    "        label = name.value\n",
    "        face_data_manager.update_label(frame_number, face['image'], label)\n",
    "        process_frame(frame_number)\n",
    "    return on_button_clicked\n",
    "\n",
    "def process_frame(frame_number):\n",
    "    print(f\"Frame number: {frame_number}\")\n",
    "    global face_data_manager\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "\n",
    "    frame_path = processed_images[frame_number]\n",
    "    frame = cv2.imread(str(frame_path))\n",
    "\n",
    "    frame_widget = convert_frame_to_widget(frame)\n",
    "    widgets_to_display = [frame_widget]\n",
    "    if 'swappings' in globals():\n",
    "        # Generate the image with swapped faces\n",
    "        faces = app.get(frame)\n",
    "        swapped_frame = face_swap(frame, faces,frame_number)\n",
    "        swapped_frame_widget = convert_frame_to_widget(swapped_frame)\n",
    "        widgets_to_display.append(swapped_frame_widget)\n",
    "\n",
    "    faces = [p for p in face_data_manager.data if p[\"frame\"] == int(frame_number)]\n",
    "    face_widgets = process_faces(faces,frame_number)\n",
    "\n",
    "    display(HBox(widgets_to_display ))\n",
    "    display(HBox(face_widgets))\n",
    "    labels = set(face['label'] for face in face_data_manager.data)\n",
    "    image_boxes = [display_images(label,faces) for label in labels]\n",
    "\n",
    "    grid = widgets.GridBox(image_boxes, layout=widgets.Layout(grid_template_columns=\"repeat(4, 250px)\"))\n",
    "    display(grid)\n",
    "\n",
    "def convert_frame_to_widget(frame):\n",
    "    frame_pil = PilImage.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    base_width = 500\n",
    "    w_percent = (base_width / float(frame_pil.size[0]))\n",
    "    h_size = int((float(frame_pil.size[1]) * float(w_percent)))\n",
    "    frame_pil = frame_pil.resize((base_width, h_size))\n",
    "\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    frame_pil.save(img_byte_arr, format='PNG')\n",
    "    return widgets.Image(value=img_byte_arr.getvalue(), format='png')\n",
    "\n",
    "def process_faces(faces, frame_number):\n",
    "    face_widgets = []\n",
    "    for face in faces:\n",
    "        if len(face_widgets) == 0:\n",
    "            print(f\"Time: {face['time']}\")\n",
    "        cropped_face_widget = convert_face_to_widget(face['image'])\n",
    "        name = widgets.Text(value=str(face['label']), placeholder='Enter character name', description='Name:')\n",
    "        button = widgets.Button(description='Confirm')\n",
    "        on_button_clicked = create_on_button_clicked(face, frame_number, name, path, character_folder)\n",
    "        button.on_click(on_button_clicked)\n",
    "        face_box = VBox([cropped_face_widget, name, button])\n",
    "        face_widgets.append(face_box)\n",
    "    return face_widgets\n",
    "\n",
    "def convert_face_to_widget(face):\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)  # Add this line to convert the color space\n",
    "    cropped_face_pil = PilImage.fromarray(face)\n",
    "    cropped_face_pil = cropped_face_pil.resize((200, 200))\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    cropped_face_pil.save(img_byte_arr, format='PNG')\n",
    "    return widgets.Image(value=img_byte_arr.getvalue(), format='jpg')\n",
    "\n",
    "def calculate_centroid(embeddings):\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "def display_images(label,frame_faces):\n",
    "    central_image ,centroid= find_central_images(label)\n",
    "    central_image_widget = convert_face_to_widget(central_image)\n",
    "\n",
    "    # Retrieve the average embedding for this label\n",
    "    label_embedding = centroid\n",
    "\n",
    "    # For each face in the current frame, calculate the similarity to this label\n",
    "    similarity_scores_widgets = []\n",
    "    # for face in frame_faces:\n",
    "    #     face_embedding = face['normed_embedding']\n",
    "    #     similarity = calculate_cosine_similarity(face_embedding, label_embedding)\n",
    "    #     sl = classify_face(face_embedding)\n",
    "    #     similarity_scores_widgets.append(widgets.Text(value=f\"{similarity:.3f}\", description=f'Similarity {sl}:', disabled=True))\n",
    "\n",
    "    return VBox([widgets.Label(f\"Label: {label}\"), central_image_widget] + similarity_scores_widgets)\n",
    "\n",
    "def find_central_images(label):\n",
    "    label_embeddings = [d['normed_embedding'] for d in face_data_manager.data if d['label'] == str(label)]\n",
    "    centroid = calculate_centroid(label_embeddings)\n",
    "    distances = [np.linalg.norm(e - centroid) for e in label_embeddings]\n",
    "    min_index = np.argmin(distances)\n",
    "    central_image = [d['image'] for d in face_data_manager.data if d['label'] == str(label)][min_index]\n",
    "    return central_image,centroid\n",
    "\n",
    "def on_slider_change(change):\n",
    "    process_frame(change['new'])\n",
    "    \n",
    "def save_swapped_image(b):\n",
    "    frame_number = slider.value  # or the current frame number\n",
    "    frame_path = processed_images[frame_number]\n",
    "    frame = cv2.imread(str(frame_path))\n",
    "\n",
    "    if 'swappings' in globals():\n",
    "        # Generate the image with swapped faces\n",
    "        faces = app.get(frame)\n",
    "        swapped_frame = face_swap(frame, faces)\n",
    "\n",
    "        # Save the swapped frame\n",
    "        save_path = os.path.join(swapped_folder, f'{frame_number}.jpg')\n",
    "        cv2.imwrite(save_path, swapped_frame)\n",
    "        print(f'Swapped image saved to {save_path}')\n",
    "def on_next_button_clicked(b):\n",
    "    label = label_input.value\n",
    "    current_frame = slider.value\n",
    "    start_frame, end_frame = range_slider.value\n",
    "    \n",
    "    next_frame = None\n",
    "    for f in face_data_manager.data:\n",
    "        if f['label'] == label and start_frame <= f['frame'] <= end_frame and f['frame'] > current_frame:\n",
    "            next_frame = f['frame']\n",
    "            break\n",
    "\n",
    "    if next_frame is not None:\n",
    "        slider.value = next_frame\n",
    "    else:\n",
    "        print(f\"No more frames with label '{label}' within the selected range from frame {start_frame} to frame {end_frame}\")\n",
    "\n",
    "\n",
    "def on_change_label_button_clicked(b):\n",
    "    old_label = old_label_field.value\n",
    "    new_label = new_label_field.value\n",
    "    start_frame, end_frame = range_slider.value\n",
    "    for face_data in face_data_manager.data:\n",
    "        if face_data['label'] == old_label and start_frame <= face_data['frame'] <= end_frame:\n",
    "            face_data['label'] = new_label\n",
    "    face_data_manager.save_data()\n",
    "def face_swap(frame, faces,frame_number):\n",
    "\n",
    "    global targets\n",
    "    global unused_keys\n",
    "    res = frame.copy()\n",
    "    used_indices = set()  # Keep track of used indices in this frame\n",
    "    face_info = []  # List to store face info (index and position)\n",
    "    for i,face in enumerate(faces):\n",
    "        index = classify_face(face.normed_embedding)\n",
    "        used_indices.add(index)  # Mark this index as used\n",
    "        most_common_index = index\n",
    "        found = False\n",
    "        bbox = face.bbox.astype(int)\n",
    "        face_info.append((index,bbox))\n",
    "        # Store index and position for later use\n",
    "        if most_common_index in targets:\n",
    "            found = True\n",
    "            value = targets[most_common_index]\n",
    "            if value is not None:\n",
    "                res = swapper.get(res, face, swappings[value], paste_back=True)\n",
    "        if not found:\n",
    "            if most_common_index in attribution:\n",
    "                found = True\n",
    "                value = swappings[attribution[most_common_index]]\n",
    "                if value is not None:\n",
    "                    res = swapper.get(res, face, swappings[attribution[most_common_index]], paste_back=True)\n",
    "            # else:\n",
    "            #     print(\"index not found :\" ,most_common_index)\n",
    "        if not found and index != \"Unknown\":\n",
    "            if len(unused_keys) == 0:  \n",
    "                unused_keys = list(swappings.keys())\n",
    "                for key in list(targets.keys()):\n",
    "                    if key in unused_keys:\n",
    "                        unused_keys.remove(key)\n",
    "            random_key = random.choice(unused_keys)\n",
    "            unused_keys.remove(random_key)\n",
    "            # print(\"index \",most_common_index,\" attribute to \",random_key, \" frame \", frame_count)\n",
    "            attribution[most_common_index] = random_key\n",
    "            random_value = swappings[random_key]\n",
    "            res = swapper.get(res, face, random_value, paste_back=True)\n",
    "        debug = res\n",
    "        for info in face_info:\n",
    "            index, bbox = info\n",
    "            new_bbox = bbox \n",
    "            cv2.putText(debug, str(index), (int(new_bbox[0]), int(new_bbox[1])), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "        debug =  cv2.resize(debug,  (400, 400), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "        cv2.putText(debug,str(frame_count), (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(debug,str(len(face_info)), (350, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "        results.append(debug)\n",
    "    return res\n",
    "\n",
    "# -------------------------------- Execution starts here ------------------------------\n",
    "cap = cv2.VideoCapture(video_output)\n",
    "total_frames = int(len(processed_images))\n",
    "\n",
    "\n",
    "first_frame_with_label_0 = min([f['frame'] for f in face_data_manager.data if f['label'] == '0' or f['label'] == ''], default=0)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "save_button = widgets.Button(description='Save Swapped Image')\n",
    "save_button.on_click(save_swapped_image)\n",
    "\n",
    "display(save_button)\n",
    "\n",
    "slider = widgets.IntSlider(min=0, max=int(total_frames-1), step=fps/4, description='Frame:')\n",
    "slider.observe(on_slider_change, names='value')\n",
    "\n",
    "label_input = widgets.Text(placeholder='Enter character label', description='Label:')\n",
    "next_button = widgets.Button(description='Go to next')\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "\n",
    "display(HBox([label_input, next_button]))\n",
    "\n",
    "old_label_field = widgets.Text(value='', placeholder='Enter old label', description='Old Label:')\n",
    "new_label_field = widgets.Text(value='', placeholder='Enter new label', description='New Label:')\n",
    "change_label_button = widgets.Button(description='Change Label')\n",
    "change_label_button.on_click(on_change_label_button_clicked)\n",
    "\n",
    "range_slider = widgets.IntRangeSlider(value=[0, total_frames-1], min=0, max=total_frames-1, step=fps, description='Frame Range:',)\n",
    "input_widgets = widgets.HBox([range_slider, old_label_field, new_label_field, change_label_button])\n",
    "\n",
    "display(input_widgets)\n",
    "display(slider)\n",
    "\n",
    "slider.value = 0\n",
    "display(frame_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_images = glob.glob(os.path.join(swapped_folder, '0*.jpg'))\n",
    "for file in processed_images:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "output_video = \"/notebooks/output.mp4\"\n",
    "video = cv2.VideoCapture(video_output)\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "video.release()\n",
    "\n",
    "# Get the processed images\n",
    "processed_images = glob.glob(os.path.join(swapped_folder, '*.jpg'))\n",
    "\n",
    "# Sort the processed images (this may be necessary depending on how your files are named)\n",
    "processed_images.sort()\n",
    "print(len(processed_images))\n",
    "# Initialize the video writer\n",
    "height, width, _ = cv2.imread(processed_images[0]).shape\n",
    "print(height, width)\n",
    "\n",
    "# Define the command\n",
    "command = f'ffmpeg -y -r {fps} -s {width}x{height} -i {swapped_folder}/%01d.jpg -vcodec libx264  -pix_fmt yuv420p {output_video}'\n",
    "\n",
    "# Execute the command\n",
    "subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(output_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\"\"\" from pydub import AudioSegment\n",
    "\n",
    "def convert_wav_to_aac(wav_file_path, aac_file_path):\n",
    "    audio = AudioSegment.from_wav(wav_file_path)\n",
    "    audio.export(aac_file_path, format=\"mp3\") \"\"\"\n",
    "\n",
    "# Call the function with your files\n",
    "\n",
    "\"\"\" convert_wav_to_aac('output.wav', 'output.mp3') \"\"\"\n",
    "temp_audio_file = 'output.aac'\n",
    "\n",
    "# Remove temporary audio file if it exists\n",
    "if os.path.exists(temp_audio_file):\n",
    "    os.remove(temp_audio_file)\n",
    "\n",
    "# Remove output video with audio file if it exists\n",
    "output_with_audio_file = '/notebooks/output_with_audio.mp4'\n",
    "if os.path.exists(output_with_audio_file):\n",
    "    os.remove(output_with_audio_file)\n",
    "\n",
    "# Extract audio from original video and save it as a temporary audio file\n",
    "audio_extraction_command = f'ffmpeg -y -i {video_output} -vn -acodec aac -strict -2 {temp_audio_file}'\n",
    "subprocess.run(audio_extraction_command, shell=True)\n",
    "\n",
    "# Combine swapped video with original audio\n",
    "video_combination_command = f'ffmpeg -y -i {output_video} -i {temp_audio_file} -c:v copy -c:a copy -map 0:v:0 -map 1:a:0 {output_with_audio_file}'\n",
    "subprocess.run(video_combination_command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHISPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade git+https://github.com/coqui-ai/TTS torch torchvision   pydub tensorflow soundfile openai-whisper librosa git+https://github.com/huggingface/transformers.git speechbrain sentencepiece\n",
    "%pip install git+https://github.com/espnet/espnet\n",
    "%pip install -q espnet_model_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/coqui-ai/TTS\n",
    "%cd /notebooks/TTS\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /notebooks/TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example voice cloning with YourTTS in English, French and Portuguese\n",
    "from TTS.api import TTS\n",
    "\n",
    "from moviepy.editor import AudioFileClip\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from IPython.display import display, Audio\n",
    "from bark import SAMPLE_RATE, generate_audio, preload_models\n",
    "from scipy.io.wavfile import write as write_wav\n",
    "from IPython.display import Audio\n",
    "\"\"\" \n",
    "tts = TTS(model_name=\"tts_models/multilingual/multi-dataset/your_tts\", progress_bar=False, gpu=True)\n",
    "tts.tts_to_file(\"Salut c'est louis en vlog.L'Alter Real est une lignée du cheval Lusitanien considérée comme particulièrement pure, généralement décrite comme une race de chevaux de selle d'origine portugaise à part entière. La sélection de cette lignée commence au xviiie siècle, avec un grand succès grâce à l'implication de la famille royale portugaise et à son usage par des écuyers renommés. L'Alter Real connaît une éclipse durant le siècle suivant, puis est sévèrement menacé de disparition et sauvé par le Dr Ruy d'Andrade. Son élevage est depuis soutenu par le gouvernement portugais. Très proche du Lusitanien avec lequel il est confondu, mais aussi du Pure race espagnole et du Minorquin, l'Alter Real est un excellent cheval de dressage, plusieurs fois représenté aux Jeux olympiques. Il présente très souvent une robe baie. \", speaker_wav=\"audio.wav\", language=\"fr-fr\", file_path=\"cloning_tts.wav\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.api import TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts = TTS(model_name=\"voice_conversion_models/multilingual/vctk/freevc24\", progress_bar=False, gpu=True)\n",
    "tts.voice_conversion_to_file(source_wav=\"upgrade.wav\", target_wav=\"target-upgrade.wav\", file_path=\"output.wav\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_waveform, input_sampling_rate= librosa.load('output.wav', sr=None)\n",
    "Audio(input_waveform, rate=input_sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# load audio and pad/trim it to fit 30 seconds\n",
    "audio = whisper.load_audio(\"/notebooks/temp_audio.aac\")\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "# decode the audio\n",
    "options = whisper.DecodingOptions(without_timestamps=False)\n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "# print the recognized text\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import AudioFileClip\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "def extract_audio_from_video(video_file_path , output_file_path,start_time = None, end_time= None, sr=16000):\n",
    "    audio = AudioFileClip(video_file_path)\n",
    "    if  start_time is not  None and end_time is not  None:\n",
    "        audio = audio.subclip(start_time, end_time)\n",
    "    audio.write_audiofile(output_file_path)\n",
    "        # Save the audio with the original sampling rate\n",
    "    temp_file_path = \"temp_audio.wav\"\n",
    "    audio.write_audiofile(temp_file_path)\n",
    "    \n",
    "    # Load the audio with the original sampling rate\n",
    "    signal, sr_orig = librosa.load(temp_file_path, sr=None)\n",
    "    if len(signal.shape) > 1:\n",
    "        signal = librosa.to_mono(signal.T)    \n",
    "    # Resample the audio to 16kHz\n",
    "    signal_resampled = librosa.resample(signal, orig_sr=sr_orig, target_sr=sr)\n",
    "    \n",
    "    # Save the resampled audio\n",
    "    sf.write(output_file_path, signal_resampled, sr)\n",
    "\n",
    "# Call the function with your files\n",
    "extract_audio_from_video('Tamioc, le nouveau podcast du Pacifique.mp4', output_file_path='audio.wav')\n",
    "extract_audio_from_video(video_output, output_file_path='temp_audio.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "\n",
    "\n",
    "def generate_speaker_embedding(audio_file_path, model_name=\"speechbrain/spkrec-xvect-voxceleb\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    classifier = EncoderClassifier.from_hparams(source=model_name, run_opts={\"device\": device})\n",
    "\n",
    "    # Load the audio file\n",
    "    signal, fs = torchaudio.load(audio_file_path)\n",
    "\n",
    "    assert fs == 16000, fs\n",
    "\n",
    "    # Generate the speaker embedding\n",
    "    with torch.no_grad():\n",
    "        embeddings = classifier.encode_batch(signal)\n",
    "        embeddings = F.normalize(embeddings, dim=2)\n",
    "        embeddings = embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Call the function with your audio file\n",
    "embedding = generate_speaker_embedding('audio.wav')\n",
    "\n",
    "# Save the embedding to a numpy file\n",
    "np.save('speaker.npy', embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import SpeechT5Processor, SpeechT5ForSpeechToSpeech, SpeechT5HifiGan\n",
    "import soundfile as sf\n",
    "from datasets import load_dataset\n",
    "\n",
    "checkpoint = \"microsoft/speecht5_vc\"\n",
    "processor = SpeechT5Processor.from_pretrained(checkpoint)\n",
    "model = SpeechT5ForSpeechToSpeech.from_pretrained(\"microsoft/speecht5_vc\")\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "\n",
    "def process_audio(sampling_rate, waveform):\n",
    "    if len(waveform.shape) > 1:\n",
    "        waveform = librosa.to_mono(waveform.T)\n",
    "\n",
    "    if sampling_rate != 16000:\n",
    "        waveform = librosa.resample(waveform, orig_sr=sampling_rate, target_sr=16000)\n",
    "\n",
    "    waveform = waveform\n",
    "    waveform = torch.tensor(waveform)\n",
    "    return waveform\n",
    "\n",
    "def voice_conversion(input_audio_path, speaker_audio_path, output_audio_path):\n",
    "    # Load and process the input audio\n",
    "    input_waveform, input_sampling_rate= librosa.load(input_audio_path, sr=None)\n",
    "    input_waveform = process_audio(input_sampling_rate, input_waveform)\n",
    "\n",
    "    # Load the speaker embedding\n",
    "    speaker_embedding = np.load(speaker_audio_path)\n",
    "    speaker_embedding = torch.tensor(speaker_embedding).unsqueeze(0)  # Remove the first dimension\n",
    "    inputs = processor(audio=input_waveform, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    \n",
    "    # Perform the voice conversion\n",
    "    speech = model.generate_speech(inputs[\"input_values\"], speaker_embeddings = speaker_embedding, vocoder=vocoder)\n",
    "\n",
    "    \n",
    "    # Write the output audio to a file\n",
    "    sf.write(output_audio_path, speech, 16000)\n",
    "\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "dataset = dataset.sort(\"id\")\n",
    "example = dataset[40]\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "np.save(\"test.npy\",torch.tensor(embeddings_dataset[7306][\"xvector\"]))\n",
    "sf.write(\"test.wav\",example[\"audio\"][\"array\"], 16000)\n",
    "\n",
    "# Call the function with your files\n",
    "voice_conversion('upgrade.wav', 'bark_voices/louis/speaker.npz', 'output_audio.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_waveform, input_sampling_rate= librosa.load('output.wav', sr=None)\n",
    "Audio(input_waveform, rate=input_sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "sampling_rate = dataset.features[\"audio\"].sampling_rate\n",
    "inputs = processor(audio=example[\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "speech = model.generate_speech(inputs[\"input_values\"], speaker_embeddings, vocoder=vocoder)\n",
    "Audio(speech, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import soundfile\n",
    "from IPython.display import display, Audio\n",
    "from TTS.api import TTS\n",
    "mixwav_mc, sr = soundfile.read(\"audio.wav\")\n",
    "# mixwav.shape: num_samples, num_channels\n",
    "mixwav_sc = mixwav_mc\n",
    "display(Audio(mixwav_mc, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from espnet_model_zoo.downloader import ModelDownloader\n",
    "\n",
    "d = ModelDownloader()\n",
    "cfg = d.download_and_unpack(\"espnet/Wangyou_Zhang_chime4_enh_train_enh_conv_tasnet_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "# If you encounter error \"No module named 'espnet2'\", please re-run the 1st Cell. This might be a colab bug.\n",
    "import sys\n",
    "import soundfile\n",
    "from espnet2.bin.enh_inference import SeparateSpeech\n",
    "\n",
    "\n",
    "separate_speech = {}\n",
    "# For models downloaded from GoogleDrive, you can use the following script:\n",
    "enh_model_sc = SeparateSpeech(\n",
    "  train_config=cfg[\"train_config\"],\n",
    "  model_file=cfg[\"model_file\"],\n",
    "  # for segment-wise process on long speech\n",
    "  normalize_segment_scale=False,\n",
    "  show_progressbar=True,\n",
    "  ref_channel=4,\n",
    "  normalize_output_wav=True,\n",
    "  device=\"cuda:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import soundfile as soundfile\n",
    "\n",
    "mixwav_mc, sr = soundfile.read(\"temp_audio.wav\")\n",
    "# mixwav.shape: num_samples, num_channels\n",
    "mixwav_sc = mixwav_mc\n",
    "wave = enh_model_sc(mixwav_sc[None, ...], sr)\n",
    "sf.write(\"upgrade.wav\",wave[0].squeeze(), sr)\n",
    "display(Audio(wave[0].squeeze(), rate=sr))\n",
    "\n",
    "\n",
    "mixwav_mc, sr = soundfile.read(\"bark_voices/gael/speaker.wav\")\n",
    "\"\"\" if sr != 16000:\n",
    "    mixwav_mc = librosa.resample(mixwav_mc, orig_sr=sr, target_sr=16000)\n",
    "if len(mixwav_mc.shape) > 1:\n",
    "    mixwav_mc = librosa.to_mono(mixwav_mc.T)  \"\"\"\n",
    "# mixwav.shape: num_samples, num_channels\n",
    "mixwav_sc = mixwav_mc\n",
    "wave = enh_model_sc(mixwav_sc[None, ...], sr)\n",
    "sf.write(\"bark_voices/gael/speaker.wav\",wave[0].squeeze(), sr)\n",
    "display(Audio(wave[0].squeeze(), rate=sr))\n",
    "\n",
    "\"\"\" mixwav_mc, sr = soundfile.read(\"output.wav\")\n",
    "# mixwav.shape: num_samples, num_channels\n",
    "mixwav_sc = mixwav_mc\n",
    "wave = enh_model_sc(mixwav_sc[None, ...], sr)\n",
    "sf.write(\"output-upgrade.wav\",wave[0].squeeze(), sr)\n",
    "display(Audio(wave[0].squeeze(), rate=sr)) \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Dans un village charmant au bord de la mer, vivait une famille extraordinaire.\n",
    "\"\"\"\n",
    "from TTS.tts.configs.bark_config import BarkConfig\n",
    "from TTS.tts.models.bark import Bark\n",
    "\n",
    "config = BarkConfig()\n",
    "model = Bark.init_from_config(config)\n",
    "model.load_checkpoint(config, checkpoint_dir=\"/root/.local/share/tts/tts_models--multilingual--multi-dataset--bark\", eval=True)\n",
    "\n",
    "\"\"\" # with random speaker\n",
    "output_dict = model.synthesize(text, config, speaker_id=\"random\", voice_dirs=None)\n",
    " \"\"\"\n",
    "# cloning a speaker.\n",
    "# It assumes that you have a speaker file in `bark_voices/speaker_n/speaker.wav` or `bark_voices/speaker_n/speaker.npz`\n",
    "output_dict = model.synthesize(text, config, speaker_id=\"gael\", voice_dirs=\"bark_voices/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.api import TTS\n",
    "\n",
    "# Load the model to GPU\n",
    "# Bark is really slow on CPU, so we recommend using GPU.\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/bark\", gpu=True) \n",
    "\n",
    "# Cloning a new speaker\n",
    "# This expects to find a mp3 or wav file like `bark_voices/new_speaker/speaker.wav`\n",
    "# It computes the cloning values and stores in `bark_voices/new_speaker/speaker.npz`\n",
    "\"\"\" tts.tts_to_file(text=\"Hello, how are you?\",\n",
    "                file_path=\"output.wav\",\n",
    "                voice_dir=\"bark_voices/\") \"\"\"\n",
    "\n",
    "\n",
    "# When you run it again it uses the stored values to generate the voice.\n",
    "tts.tts_to_file(text=\"\"\"\n",
    "Dans un village charmant au bord de la mer, vivait une famille extraordinaire. Le père, l'architecte ambitieux, avait une voix grave et rauque. La mère, la pianiste accomplie, parlait avec douceur et finesse. Leur fille unique, une poétesse talentueuse, avait une voix mélodieuse.\n",
    "\n",
    "Un jour d'été, une lettre mystérieuse est arrivée. \"Chère famille, une aventure incroyable vous attend à l'autre bout du monde. Montez à bord du bateau qui quitte le port demain à l'aube, et laissez-vous guider par le vent et les vagues.\"\n",
    "\n",
    "Emballés par l'idée, ils ont fait leurs valises. Ils ont rencontré des gens fantastiques lors de leur voyage : un marin à la voix rauque, une conteuse à la voix douce et un enfant qui chantait avec une voix cristalline. Chaque personne qu'ils ont rencontrée avait une histoire à raconter.               \n",
    "\"\"\",\n",
    "                file_path=\"output.wav\",\n",
    "                voice_dir=\"bark_voices\",\n",
    "                speaker=\"gael\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "def convert_m4a_to_wav(m4a_filepath, wav_filepath):\n",
    "    audio = AudioSegment.from_file(m4a_filepath, format=\"m4a\")\n",
    "    audio = audio.set_frame_rate(SAMPLE_RATE)\n",
    "    audio = audio.split_to_mono()[0]\n",
    "    audio.export(wav_filepath, format=\"wav\")\n",
    "def convert_ogg_to_wav(ogg_file_path, wav_file_path):\n",
    "    audio = AudioSegment.from_ogg(ogg_file_path)\n",
    "    audio.export(wav_file_path, format=\"wav\")\n",
    "\n",
    "# usage\n",
    "convert_m4a_to_wav(\"/notebooks/20230715_173011.m4a\", \"bark_voices/gael/speaker.wav\")\n",
    "mixwav_mc, sr = soundfile.read(\"bark_voices/gael/speaker.wav\")\n",
    "print(sr)\n",
    "display(Audio(\"bark_voices/gael/speaker.wav\", rate=sr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tts --list_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /root/.local/share/tts/tts_models--multilingual--multi-dataset--xtts_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepfilternet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!deepFilter louis.ogg -o louis.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.api import TTS\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v1\").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts.tts_to_file(\"C'est juste pour découvrir la room\", speaker_wav=\"test2.wav\", language=\"fr\", file_path=\"output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Audio\n",
    "Audio(\"output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tts = TTS(\"tts_models/en/thorsten/tacotron2-DDC\")\n",
    "# tts.tts_with_vc_to_file(\n",
    "# \"\"\"\n",
    "# Bonjour à tous ! Bienvenue sur ma chaîne, je suis Louis en vlog, votre guide local, directement depuis la splendide Nouvelle-Calédonie. Ici, on explore, on découvre et on s'amuse sous le soleil du Pacifique ! Préparez-vous à plonger dans les eaux cristallines, à grimper les sommets verdoyants et à vivre des rencontres inoubliables avec notre incroyable communauté. Ensemble, nous allons voyager à travers ce territoire de légendes, et qui sait, peut-être même que nous dévoilerons quelques-uns de ses secrets... Alors, attachez bien votre ceinture, c'est parti pour une nouvelle aventure ! Et n'oubliez pas, si vous aimez ce que vous voyez, cliquez sur 'J'aime', partagez et abonnez-vous pour ne rien manquer. C'est parti !\n",
    "# \"\"\",\n",
    "#     speaker_wav=\"target-upgrade.wav\",\n",
    "#     file_path=\"output.wav\"\n",
    "# )\n",
    "\n",
    "# from TTS.api import TTS\n",
    "# api = TTS(model_name=\"tts_models/eng/fairseq/vits\", gpu=True)\n",
    "# api.tts_to_file(\"This is a test.\", file_path=\"output.wav\")\n",
    "\n",
    "# TTS with on the fly voice conversion\n",
    "api = TTS(\"tts_models/multilingual/multi-dataset/bark\", gpu=True)\n",
    "api.tts_with_vc_to_file(\n",
    "\"\"\"\n",
    "[laughs] Y'a pas sport demain,j'imagine ? [sighs]\n",
    "\"\"\",\n",
    "    speaker_wav=\"bark_voices/gael/speaker.wav\",\n",
    "    file_path=\"output.wav\"\n",
    ")\n",
    "# play text in notebook\n",
    "Audio(\"output.wav\", rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bark import SAMPLE_RATE, generate_audio, preload_models\n",
    "from scipy.io.wavfile import write as write_wav\n",
    "from IPython.display import Audio\n",
    "preload_models()\n",
    "# download and load all models\n",
    "# preload_models()\n",
    "\n",
    "# # generate audio from text\n",
    "# text_prompt = \"\"\"\n",
    "#      Hello, my name is Suno. And, uh — and I like pizza. [laughs] \n",
    "#      But I also have other interests such as playing tic tac toe.\n",
    "# \"\"\"\n",
    "# audio_array = generate_audio(text_prompt, history_prompt=\"bark_voices/louis\")\n",
    "\n",
    "# # save audio to disk\n",
    "# write_wav(\"bark_generation.wav\", SAMPLE_RATE, audio_array)\n",
    "  \n",
    "# # play text in notebook\n",
    "# Audio(audio_array, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate audio from text\n",
    "text_prompt = \"\"\"\n",
    "[laughs] Y'a pas sport demain,j'imagine ? [sighs]\n",
    "\"\"\"\n",
    "audio_array = generate_audio(text_prompt, history_prompt=\"v2/fr_speaker_0\")\n",
    "\n",
    "# save audio to disk\n",
    "write_wav(\"bark_generation.wav\", SAMPLE_RATE, audio_array)\n",
    "  \n",
    "# play text in notebook\n",
    "Audio(audio_array, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/suno-ai/bark.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bark import SAMPLE_RATE, generate_audio, preload_models\n",
    "from IPython.display import Audio\n",
    "\n",
    "preload_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompt ='''\n",
    "♪D'où viens-tu, petite tache noire ? \n",
    "Est-ce l'Éthiopie ou le Salvador ?\n",
    "J'attends, tasse à la main,♪ \n",
    "Sers-moi encore. [soupir satisfait]\n",
    "Café, où es-tu ? [ton désespéré]\n",
    "Macchiato, où es-tu ? [en recherche]\n",
    "Cappuccino, où es-tu ? [nostalgique]\n",
    "Café, où es-tu ? Où es-tu ? Où es-tu ?♪ [urgence croissante]\n",
    "'''\n",
    "audio_array = generate_audio(text_prompt, history_prompt=\"v2/fr_speaker_3\")\n",
    "Audio(audio_array, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompt ='''\n",
    "(Verse 1)\n",
    "♪ D'où viens-tu, petite tache noire ? ♪\n",
    "♪ Est-ce l'Éthiopie ou le Salvador ? ♪\n",
    "♪ J'attends, tasse à la main, ♪\n",
    "♪ Sers-moi encore. ♪          \n",
    "'''\n",
    "audio_array2 = generate_audio(text_prompt, history_prompt=\"v2/fr_speaker_0\")\n",
    "Audio(audio_array, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, BarkModel\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"suno/bark\")\n",
    "model = BarkModel.from_pretrained(\"suno/bark\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "voice_preset = \"v2/fr_speaker_6\"\n",
    "inputs = processor('''\n",
    "(Verse 1)\n",
    "♪ D'où viens-tu, petite tache noire ?\n",
    "Est-ce l'Éthiopie ou le Salvador ?\n",
    "J'attends, tasse à la main,\n",
    "Sers-moi encore. ♪\n",
    "              \n",
    "''', voice_preset=voice_preset)\n",
    "\n",
    "audio_array = model.generate(**inputs)\n",
    "audio_array = audio_array.cpu().numpy().squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "sample_rate = model.generation_config.sample_rate\n",
    "Audio(audio_array, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "sample_rate = model.generation_config.sample_rate\n",
    "scipy.io.wavfile.write(\"bark_out.wav\", rate=sample_rate, data=audio_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faceswap",
   "language": "python",
   "name": "faceswap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
