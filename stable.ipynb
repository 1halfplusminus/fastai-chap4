{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install accelerate\n",
    "%pip install safetensors\n",
    "%pip install invisible-watermark>=0.2.0\n",
    "%pip install omegaconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/huggingface/diffusers.git@main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://civitai.com/api/download/models/148259 -O DynaVision_XL.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://civitai.com/api/download/models/129610 -O 3d_style_lora.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://civitai.com/api/download/models/127055/ -O anime_art_xl.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://civitai.com/api/download/models/94057 -O fastnegativeV2.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://civitai.com/api/download/models/126688 -O dreamshaper.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://civitai.com/api/download/models/144566 -O samaritan3dCartoon.safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline, UniPCMultistepScheduler,DiffusionPipeline,StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# base = StableDiffusionXLPipeline.from_pretrained(\n",
    "#      \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    "# )\n",
    "base = StableDiffusionXLPipeline.from_single_file(\n",
    "     \"./DynaVision_XL.safetensors\", \n",
    "     torch_dtype=torch.float16, \n",
    "     variant=\"fp16\", \n",
    "     use_safetensors=True,\n",
    "    scheduler=\"sample_dpmpp_2m_sde\"\n",
    ")\n",
    "\n",
    "base.to(\"cuda\")\n",
    "# base.load_lora_weights(\"./3d_style_lora.safetensors\")\n",
    "base.load_textual_inversion(\"./fastnegativeV2.pt\")\n",
    "refiner = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    text_encoder_2=base.text_encoder_2,\n",
    "    vae=base.vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\"\n",
    ")\n",
    "refiner.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 35\n",
    "high_noise_frac = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'''\n",
    "high quality amazing detailed art of \n",
    "an 3D stylized cute little werewolf boy,\n",
    "full body reference image for sculting\n",
    "'''\n",
    "neg_prompt =f''' \n",
    "FastNegativeV2,low res, low quality, bad detail, asymmetry,text\n",
    "'''\n",
    "image = base(\n",
    "    prompt=prompt,\n",
    "  num_inference_steps=n_steps,\n",
    "    # denoising_end=high_noise_frac,\n",
    "    # output_type=\"latent\",\n",
    "    negative_prompt=neg_prompt\n",
    ").images[0]\n",
    "image.show()\n",
    "# image = base(\n",
    "#     prompt=prompt,\n",
    "#     num_inference_steps=n_steps,\n",
    "#     denoising_end=high_noise_frac,\n",
    "#     output_type=\"latent\",\n",
    "#     negative_prompt=neg_prompt\n",
    "# ).images\n",
    "\n",
    "# image = refiner(\n",
    "#     prompt=prompt,\n",
    "#     num_inference_steps=n_steps,\n",
    "#     denoising_start=high_noise_frac,\n",
    "#     image=image,\n",
    "#     negative_prompt=neg_prompt\n",
    "# ).images[0]\n",
    "# image.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"werewolf\"\n",
    "image.save(f\"/notebooks/threestudio/load/images/{name}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONTROL NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL\n",
    "from diffusers.utils import load_image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", controlnet=controlnet, vae=vae, torch_dtype=torch.float16\n",
    ")\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"an dragon\"\n",
    "negative_prompt = \"low quality, bad quality, sketches\"\n",
    "\n",
    "\n",
    "\n",
    "# initialize the models and pipeline\n",
    "controlnet_conditioning_scale = 0.1  # recommended for good generalization\n",
    "# download an image\n",
    "image = Image.open(\"/notebooks/front.png\")\n",
    "# get canny image\n",
    "image = np.array(image)\n",
    "image = cv2.Canny(image, 100, 200)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image)\n",
    "canny_image.show()\n",
    "# generate image\n",
    "image = pipe(\n",
    "    prompt, controlnet_conditioning_scale=controlnet_conditioning_scale, image=canny_image\n",
    ").images[0]\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import DPTFeatureExtractor, DPTForDepthEstimation\n",
    "from diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline, AutoencoderKL\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "\n",
    "depth_estimator = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\").to(\"cuda\")\n",
    "feature_extractor = DPTFeatureExtractor.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"diffusers/controlnet-depth-sdxl-1.0\",\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"Linaqruf/animagine-xl\",\n",
    "    controlnet=controlnet,\n",
    "    vae=vae,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "# pipe.enable_model_cpu_offload()\n",
    "\n",
    "def get_depth_map(image):\n",
    "    image = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(\"cuda\")\n",
    "    with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "        depth_map = depth_estimator(image).predicted_depth\n",
    "\n",
    "    depth_map = torch.nn.functional.interpolate(\n",
    "        depth_map.unsqueeze(1),\n",
    "        size=(1024, 1024),\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)\n",
    "    depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)\n",
    "    depth_map = (depth_map - depth_min) / (depth_max - depth_min)\n",
    "    image = torch.cat([depth_map] * 3, dim=1)\n",
    "\n",
    "    image = image.permute(0, 2, 3, 1).cpu().numpy()[0]\n",
    "    image = Image.fromarray((image * 255.0).clip(0, 255).astype(np.uint8))\n",
    "    return image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import DPTFeatureExtractor, DPTForDepthEstimation\n",
    "from diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline, DiffusionPipeline\n",
    "from diffusers.utils import load_image\n",
    "refiner = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    text_encoder_2=pipe.text_encoder_2,\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\"\n",
    ")\n",
    "\"\"\" refiner.load_lora_weights(\"./3d_style_lora.safetensors\") \"\"\"\n",
    "refiner.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "high_noise_frac = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = f'''\n",
    "an cat like pokemon,masterpiece, best quality,in forest\n",
    "'''\n",
    "neg_prompt =f''' \n",
    "lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\n",
    "'''\n",
    "image = Image.open(\"/notebooks/front.png\").convert(\"RGB\")\n",
    "controlnet_conditioning_scale = 0.6  # recommended for good generalization\n",
    "\n",
    "depth_image = get_depth_map(image)\n",
    "\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    negative_prompt=neg_prompt, \n",
    "    image=depth_image,\n",
    "    num_inference_steps=n_steps,\n",
    "    # denoising_start=high_noise_frac,\n",
    "    output_type=\"latent\",\n",
    "    controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "    \n",
    ").images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = refiner(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=n_steps,\n",
    "#    denoising_start=high_noise_frac,\n",
    "    image=image,\n",
    "    negative_prompt=neg_prompt\n",
    ").images[0]\n",
    "image.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /notebooks/ComfyUI\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable",
   "language": "python",
   "name": "stable"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
